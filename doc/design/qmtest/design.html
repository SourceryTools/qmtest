<html>
<head>
<title>
Software Carpentry Testing Tool Design
</title>
</head>
<body>

<h1 align="center">
Software Carpentry
<br>
Testing Tool Design
</h1>

<blockquote role="abstract"><em>
This document describes the design of the Software Carpentry testing
tool, provisionally named "Qmtest". 
</em></blockquote>

<a name="toc">
<h2>Contents</h2>
<ul>
<li><a href="#intro"> Introduction </a>
	<ul>
	<li><a href="#intro-not"> What Qmtest is Not </a></li>
	<li><a href="#intro-ack"> Acknowledgments </a></li>
	</ul>
</li>
<li><a href="#story"> User Stories </a>
	<ul>
	<li><a href="#story-bhargan"> Bhargan Basepair </a></li>
	<li><a href="#story-daryl"> Daryl Dowhile </a></li>
	<li><a href="#story-glenda"> Glenda Grammar </a></li>
	<li><a href="#story-ovide"> Ovide Overlay </a></li>
	<li><a href="#story-tahura"> Tahura Transparency </a></li>
	</ul>
</li>
<li><a href="#term"> Terminology </a></li>
<li><a href="#req"> Requirements </a>
	<ul>
	<li><a href="#req-accuracy"> Accuracy </a></li>
	<li><a href="#req-smallscale"> Small-Scale Usability </a></li>
	<li><a href="#req-reproducible"> Reproducibility </a></li>
	<li><a href="#req-standards"> Recycle Existing Standards </a></li>
	<li><a href="#req-dep"> Dependent and Independent Tests </a></li>
	<li><a href="#req-expect"> Expected and Unexpected Results </a></li>
	<li><a href="#req-report"> Reporting </a></li>
	<li><a href="#req-regen"> Regeneration of Reference Results </a></li>
	<li><a href="#req-suite"> Test Suites </a></li>
	<li><a href="#req-id"> Test Identification </a></li>
	<li><a href="#req-summary"> Summary Reporting </a></li>
	<li><a href="#req-info"> Extra Information </a></li>
	<li><a href="#req-change"> Logging Changes </a></li>
	<li><a href="#req-traversal"> Traversal </a></li>
	<li><a href="#req-physical"> Physical Organization of Tests </a></li>
	<li><a href="#req-ctrl"> Controlling Execution </a></li>
	<li><a href="#req-spec"> Test Specification </a></li>
	<li><a href="#req-multi"> Multi-Tests and Reflective Tests </a></li>
	<li><a href="#req-updown"> Setup and Teardown </a></li>
	<li><a href="#req-sandbox"> Sandboxing and Throttling </a></li>
	<li><a href="#req-list"> Requirements Summary </a></li>
	</ul>
</li>
<li><a href="#iface"> Interface </a></li>
<li><a href="#solve"> User Solutions </a>
	<ul>
	<li><a href="#solve-bhargan"> Bhargan Basepair </a></li>
	<li><a href="#solve-daryl"> Daryl Dowhile </a></li>
	<li><a href="#solve-glenda"> Glenda Grammar </a></li>
	<li><a href="#solve-ovide"> Ovide Overlay </a></li>
	<li><a href="#solve-tahura"> Tahura Transparency </a></li>
	</ul>
</li>
<li><a href="#arch"> Architecture </a></li>
<li><a href="#impl"> Implementation </a></li>
<li><a href="#devplan"> Development Plan </a></li>
<li><a href="#risks"> Risks </a></li>
<li><a href="#ref"> References </a></li>
<li><a href="#notes"> Notes </a></li>
<li><a href="#questions"> Questions </a></li>
</ul>
</a>

<a name="intro">
<h2>Introduction</h2>

<p>
Most programmers don't do enough testing today because:
<ul>
<li>
They aren't required to.
</li>
<li>
It's tedious.
</li>
<li>
Existing tools are obscure, hard to use, expensive, don't actually provide much help, or all three.
</li>
<li>
They don't know where to start, when to stop, or how to tell whether the tests they've written are meaningful.
</li>
</ul>
</p>

<p>
Software tools alone cannot solve the first problem, but Qmtest tries
to solve the second by addressing the third and fourth.  In
particular:
<ul>
<li>
Qmtest has a gentle learning curve, especially for developers without
software engineering training. Qmtest does this by:
	<ul>
	<li>
	being very simple to install and configure;
	</li>
	<li>
	making it very easy for developers to create an empty
	(skeletal) test suite for a project;
	</li>
	<li>
	making it equally easy for developers to create the first real
	test for a project, or to add another test once N tests have
	been written.
	</li>
	</ul>
</li>
<li>
Qmtest provides a very simple workflow, so that tests can easily and
systematically be added, modified, inspected, and summarized by
developers, managers, and other stakeholders.
</li>
<li>
Qmtest provides feedback regarding the quality and thoroughness of
testing so that developers will be able to tell how much they have
done, how much remains to be done, and how well third party modules
have been tested.
</li>
</ul>
</p>

<p>
Some of the particular scenarios that Qmtest handles are:
<ul>
<li>
Static unit testing of functions, classes, and modules in languages
such as C, Python, and Java. (These three languages are chosen as
examples because they span the range from low-level to high-level.)
</li>
<li>
Customizable reporting of test results, ranging from a single-line
command-line summary of the number of tests that passed and failed,
through to automatic creation of charts of test statistics over time.
</li>
<li>
Scriptable control of test suite execution, so that portions can be
executed selectively, executed repeatedly under different load
conditions, only executed at certain times of day, and so on. 
</li>
<li>
Parallel execution of test suites. 
</li>
</ul>
</p>

<p>
This document begins by describing <a href="#story">six typical
users</a>, whose testing needs Qmtest is designed to address.  It then
explores their <a href="#req">requirements</a> in more detail, in
order to determine the features that Qmtest must have.  Qmtest's <a
href="#iface">user interface</a> and <a href="#arch">architecture</a>
are described next, along with some details of its <a
href="#impl">implementation</a>.  These are followed by a provisional
<a href="#devplan">development plan</a> and an analysis of the <a
href="#risk">risks</a> the project faces.  The document closes with
pointers to other testing-related resources.
</p>

<a name="intro-not">
<h3>What Qmtest is Not</h3>

<p>
In order to bound the scope of this project, it is important to
be specific about what Qmtest will not try to provide.
</p>

<ol>

<li>
Qmtest is not a build system.  It will not determine which parts of
the programs being tested need to be recompiled, or which test
programs need to be re-linked.  It <em>may</em> include support for
re-running tests whose last recorded result is older than the thing
being tested.
</li>

<li>
Qmtest is not a version control system, or a relational database.  In
particular, versioning of tests will be handled by whatever version
control system developers are already using.  Similarly, Qmtest will
log the results of tests, but will rely on the capabilities of
external systems to maintain historical records.  Note, however, that
Qmtest <em>will</em> be able to retrieve information from those
archiving systems in order to create reports.
</li>

<li>
Qmtest is not a test creation wizard itself, although tools for
creating Qmtest-compatible tests will be shipped with it.
</li>

<li>
Qmtest is not a general program execution harness.  Some users may
choose to run their programs via Qmtest under non-test circumstances
(e.g. in order to obtain logging information), but Qmtest is not a
replacement for distributed load-balancing software, safe shells, or
other tools.
</li>

</ol>

</a> <!-- end intro-not -->

<a name="intro-ack">
<h3>Acknowledgments</h3>

<p>
This document has its origins in the submissions to the testing
category of the Software Carpentry design competition in the spring of
2000, and in the hundreds of messages on the Software Carpentry
discussion list in August--October of the same year.  We are grateful
to Paul Dubois (<a href="http://www.llnl.gov">Lawrence Livermore
National Laboratory</a>), Stephen Lee (<a
href="http://www.lanl.gov">Los Alamos National Laboratory</a>), Brian
Marick (of <a href="http://www.testing.com">testing.com</a>), Ken
Martin (<a href="http://www.kitware.com">Kitware</a>), and Dave Thomas
(a very <a href="http://www.pragmaticprogrammer.com">pragmatic
programmer</a>) for their input.
</p>

</a> <!-- end ack -->

</a> <!-- end intro -->

<a name="story">
<h2>Use Cases</h2>

<a name="story-bhargan">
<h3>Bhargan Basepair</h3>

<p>
Bhargan Basepair works for Genes'R'Us, a bio-technology firm with
development labs in four countries.  He and his two assistants are
developing fuzzy pattern-matching algorithms for finding similarities
between DNA records in standard databases.  Since his promotion,
Bhargan spends most of his time doing administrative and miscellaneous
tasks, in an attempt to prevent his assistants from losing focus.
</p>

<p>
As a semi-official service for other Genes'R'Us researchers, and as a
way of testing the efficacy of his group's heuristics, Bhargan runs an
overnight DNA sequence query service.  Researchers send him sequences
by electronic mail in a variety of formats (in-line, attachments, URLs
to pages behind the company firewall, etc.).  Bhargan saves these
messages in files called <code>search/a</code>, <code>search/b</code>,
and so on, then edits them to add query directives.  (As he is very
conscientious, he almost never overwrites one query with another.)
Before leaving at night, he runs a Bourne shell script which performs
a search using the contents of every file in the search directory in
turn.  The results of each search are put in a file with a name of the
form <code>search/a.out</code>.  Each search typically takes 15-20
minutes; he is sometimes not able to run all of the searches he has
received in a single night.
</p>

<p>
When Bhargan comes in the next morning, he pages through his mail
again, sending the appropriate <code>.out</code> file to each
researcher.  He then makes a list of searches that his shell script
didn't have time to get to, or which didn't run successfully
(e.g. because of format errors.  After copying these to a temporary
directory, he uses a small <code>awk</code> script to archive the
query sequence, the results (if any), the date, the databases
searched, and the search control parameters.  Periodically, he
examines this data in order to tune his search engine's parameters.
</p>

<p>
Bhargan wants to test the intern's code thoroughly before using it.
He has the source code, but the student has gone back to college, and
the documentation never quite got written.  Bhargan's concerns are:

<ul>

<li>
One person's results must <em>never</em> be sent to someone else ---
there could be legal fallout if this ever happened.
</li>

<li>
Queries should never be lost or garbled: anyone who sends a valid
query should eventually get a reply.  Bhargan has been doing the
filtering and queueing by hand, and decided to automate it because he
was making two or three errors per week; the automated service must
have a lower "dropped message" rate than this.
</li>

<li>
Each night's run of the program should append summary results to a log
file.  Previous results should not be overwritten.  Bhargan has to be
able to read and edit this file while the program is running --- he
spends a lot of time at conferences and customer sites, and due to the
time differences, this means it's often 2:00 a.m. in the office when
he logs in.
</li>

</ul>
</p>

<p>
One extra wrinkle is that the test program shouldn't send mail to real
people, or be sent mail from them.  The corporate IT department might
be willing to set up dummy user accounts for Bhargan to use in
testing, but he'll probably see the millenium roll over again before
it actually gets done.
</p>

</a> <!-- end story-bhargan -->

<a name="story-daryl">
<h3>Daryl Dowhile</h3>

<p>
Daryl did a comparative study of the efficiencies of different parsing
algorithms for his Master's degree in Computer Science at Euphoric
State University, and is now working on a contract to implement a
parser for Fortran-2000 (or "F00") for the GNU Compiler Collection
(GCC).  The parser currently contains 47,000 executable lines of C and
C++ in 350 functions, distributed among 18 files, and is expected to
triple in size by the time the project is done.  Even at this point,
the F00 parser will be less than a third the size of commercial C++
parsers; it will achieve this economy by re-using much of the parsing
framework developed for other languages.
</p>

<p>
Daryl starts an average working day by bringing his copy of the source
up to date with any overnight check-ins from colleagues working on
other modules.  He then checks his email to see whether there are any
upcoming developments that will affect him, such as any extensions to
the module used to internationalize error messages.  Once these
administrative tasks are done, he decides which part of the draft F00
standard he is going to implement next, and then writes a few new test
cases that exercise those language features.  (Some of these are
actually not legal F00, since a large part of the contract is to
improve the compiler's error messages.)
</p>

<p>
Daryl only starts adding the new feature to the parser once all of his
test cases are written and failing --- past experience has taught him
that if he writes his tests <em>after</em> implementing the feature,
he will test against his implementation, and not against the language
standard.  Writing the tests first also allows him to double-check the
error messages that the parser is generating.  According to the work
log he is keeping for this project, he typically makes three to five
changes to pre-existing features of the parser each time he adds a new
feature.
</p>

<p>
Daryl has inherited a collection of approximately 1200 tests, most
written to test the Fortran-77 and Fortran-90 parsers that are his
starting point.  Each test is run by invoking the compiler on a source
file with certain diagnostic flags enabled.  The test is considered a
pass if the compiler emits exactly the expected diagnostic messages.
</p>

<p>
A single source file may be used as a fixture in several tests --- for
example, it maybe compiled several times at different warning levels.
Control information for each test is embedded in specially-formatted
comments in the source files, which are scattered throughout the
source tree.  A simple Awk script extracts control information from
the source files, runs the parser, and prints the test name followed
by either "success" (if the test behaved as expected), "failure" (if
the test ran, but did not produce the expected output), or "error" (if
the test crashes the parser).
</p>

<p>
Daryl typically writes five or ten tests for each feature before
implementing the feature, and another five or ten during testing.  At
least once a day, he types:
</p>

<blockquote><pre>
make parser_test | grep -v "success:"
</pre></blockquote>

<p>
in the root directory of the project, in order to re-run all of the
tests, and display only those that have not succeeded.  (The Unix
shell command <code>grep&nbsp;-v</code> prints only those lines that
do not contain the specified substring.)
</p>

<p>
Daryl would like to improve the way in which tests are re-run.  In
particular, he would like a tool to collect statistics on how many
tests have been executed, with what results, so that he has a progress
log to include in his monthly progress report.  He would also like
some help generating tests: many are simple variations on a theme,
such as using eighteen different kinds of constants as array
subscripts, and he has made several errors in the tests themselves
when copying and modifying old test files.  Finally, he would like a
single command to run the entire test suite simultaneously on each of
the dozen or so different machines in the testing network.  These
machines are a mix of various versions of Unix and Microsoft Windows.
</p>

</a> <!-- end story-daryl -->

<a name="story-glenda">
<h3>Glenda Grammar</h3>

<p>
Glenda is writing a set of tutorials for a new image processing
library to pay her bills while she finishes a Ph.D. in mathematics.
She has taken several undergraduate and graduate-level programming
courses, and is reasonably proficient with C++.
</p>

<p>
Glenda's employer is a consulting company part-owned by her
supervisor.  Because the company's two dozen employees are now
scattered around the country, the company manages work using email
lists.  Messages sent to any of the lists are automatically archived,
and a collection of CGI scripts allow the messages to be accessed by
date, author, or subject.  (Work is under way to allow employees to
search the content of messages as well.)
</p>

<p>
On a good day, Glenda emails questions to the library's developers,
reads responses, updates the current text of the tutorials (which are
presently a collection of HTML pages), and writes or updates the
example programs.  Each time she changes an example, she re-runs it,
and checks its output (sometimes text, but more often an image).  If
she is satisfied, she puts the fresh output into the company's CVS
repository, then runs the new code through a small script to HTMLify
it (e.g. escape special characters, and replace tabs with an
appropriate number of spaces), and pastes the result into the
tutorial.
</p>

<p>
On a bad day, Glenda is told that there has been a change to the
library that will affect one of the tutorials.  In this case, she must
re-run each example that she thinks might be affected, check its
output against what is currently in the tutorial, and update material
accordingly.  She often sends messages to the "Bugs" list after doing
this, as she is usually the first one to notice when changes in one
section of the library expose errors in another.
</p>

<p>
Glenda has the following testing needs:

<ol>

<li>
Each time she changes an example she needs to ensure that the output
agrees with her description in the tutorial.  The output can be:
	<ul>

	<li>
	an image;
	</li>

	<li>
	"exact" text, such as a count of the number of files loaded
	and processed; or
	</li>

	<li>
	numerical text, such as smoothing coefficients.  In this case,
	changes to the program may cause small, but acceptable,
	changes to the output.
	</li>

	</ul>
</li>

<li>
She needs to ensure that her HTML-based tutorial and the example code
are properly cross-referenced: each example is referenced by at least
one tutorial, and all references to examples are valid.
</li>

</ol>
</p>

<p>
Ideally, both processes should be totally automated.
</p>

</a> <!-- end story-glenda -->

<a name="story-ovide">
<h3>Ovide Overlay</h3>

<p>
Ovide Overlay is developing new algorithms for the map overlay module
of a geographical information system.  The module takes as input two
maps, A and B.  Each map covers the same geographical area, and is
divided into non-overlapping polygons.  The module's output is the
overlay of the maps, i.e., the new map that would be generated by
drawing the input maps on transparent film, and placing them on top of
each other.  For example, if map A shows soil types, and map B shows
vegetation, the output map would show where different kinds of plants
are growing on different types of soil.
</p>

<p>
In order to simplify his studies, Ovide is using maps divided into
rectangles, whose vertices all lie on a [0...X]&times;[0...Y] grid.
His existing program implements a naive "all-against-all" overlay
algorithm, in which every rectangle in map A is tested against every
rectangle in map B.  Ovide wants to study two improvements to this
algorithm:

<ul>

<li>
The most obvious way to improve this algorithm is to make use of the
sorted order of the entries in each input list.  Clearly, two polygons
P and Q cannot overlay if the upper X coordinate of P is less than the
lower X coordinate of Q, or vice versa.  If the polygons in one map
are sorted by increasing upper X, and those in the other are sorted by
increasing lower X, this ordering can be used to limit the sweep of
the inner overlay loop.
</li>

<li>
A second optimization is to keep track of how much of the area of each
polygon has not yet been accounted for.  Suppose, for example, that a
polygon P from one map overlaps exactly two polygons Q1 and Q2 from a
second map.  Once the two output polygons have been generated, all of
the area of P has been "used up"; the program no longer needs to
consider it when calculating overlaps with other polygons from the
second map.  This could be implemented by adding an integer field to
each polygon to keep track of its unaccounted-for area.  When the area
of a polygon is exhausted, it is deleted from its list.
</li>

</ul>
</p>

</a> <!-- end story-ovide -->

<a name="story-tahura">
<h3>Tahura Transparency</h3>

<p>
Tahura is a junior professor at Euphoric State University, and
part-owner of a consulting company that is developing a new
transparency rendering library in C++.  (This is the same company that
employs <a href="#story-glenda">Glenda Grammar</a> as a technical
writer.)  The company's two dozen employees are scattered around the
country, and communicate largely via email, and through the shared
version control repository.
</p>

<p>
Tahura's main tasks are to make sure that all of the developers know
what they are supposed to be building, and that they are actually
building it.  Each developer is supposed to keep a list of current
tasks, and milestones recently achieved, on a personal web page.
Tahura checks these pages every Thursday morning, in preparation for a
regularly-scheduled conference call that afternoon.  She is also very
careful to save each work-related message three times: once in a
folder named for the developer, once in another folder named for the
project, and once by date.
</p>

<p>
Tahura is also still trying to do some technical work herself by
experimenting with new wrinkles on existing rendering algorithms using
MATLAB.  When one of these experiments looks promising, she hands the
MATLAB script to the company's developers, so that they can reproduce
the algorithm in C++.  The developers can then compare the output of
the (slow) MATLAB and (fast) C++ versions to validate their
implementation.  The output that is compared is not images, but rather
histograms of lighting intensity for each of a set of sample problems.
Once these are close enough (no more than 2% difference for any value,
and no more than 5% cumulative difference), the developers compare a
few images visually.
</p>

<p>
As part of the run-up to the next release of the library, Tahura wants
to start recording performance information during each run of the test
suite.  Her experience has been that unexpected changes in execution
times usually indicate that something has gone wrong, and her
customers consider any slowing down of the library to be a bug.  At
present, each test prints an identifying string (usually the name of
the algorithm, and a version number) and either "pass", "fail", or
"error".  She is trying to decide how to add performance information
to this: should tests print the absolute execution time, report the
percentage change from the previously-recorded time, or just print
"fail" instead of "pass" if the performance changes by too much?  She
must also find a way to deal with the fact that the test suite may be
run on several different machines: should different performance
expectations be recorded for each, or should some benchmark be run,
and its time used to scale the expected results?
</p>

</a> <!-- end story-tahura -->

</a> <!-- end story -->

<a name="term">
<h2>Terminology</h2>

<p>
A <b role="defn">test subject</b> is the thing being tested.  This
may range from a single function or procedure call, up to an entire
application.
</p>

<p>
A <b role="defn">fault</b> is a significant difference between a test
subject's actual behavior, and its desired behavior.  The main
purposes of testing are to help isolate faults, to ensure that faults
are not re-introduced, and to gauge the likely number of faults in the
subject.
</p>

<p>
A <b role="defn">test result</b> is one of the four following values:

<dl>
<dt>
<b role="defn">Pass</b>:
</dt>
<dd>
A "pass" result means that the test behaved as desired,
i.e. produced the correct result, raised the right exception,
or generated an image that differed from a reference image by
no more than a specified tolerance.
</dd>
<dt>
<b role="defn">Fail</b>:
</dt>
<dd>
A failure occurs when a test executes to completion, and
produces a result, but that result is not acceptable.  Failure
indicates that the subject of the test is broken.
</dd>
<dt>
<b role="defn">Error</b>:
</dt>
<dd>
An error result indicates that the test itself is broken.  Some
examples of this are a memory access violation occurring during the
setup of the test, or Qmtest not being able to find the reference
object against which this test's output is to be compared.  None,
some, or all of the test itself may have been executed in this case,
which has implications for the structure and behavior of <a
href="#req-updown">setup and teardown</a>.
</dd>
<dt>
<b role="defn">Deferred</b>:
</dt>
<dd>
A deferred result indicates that the test was not actually executed.
This may occur because of system load (e.g. the web server is too busy
to be used for testing), because the test suite has been configured to
skip particular tests, or (most commonly) because one or more of the
predecessors of a dependent test failed.
</dd>
</dl>
</p>

<p>
An <b role="defn">independent test</b> is one which:
<ol>
<li>
does not interact with other independent tests; and
</li>
<li>
produces exactly one of the four results described below.
</li>
</ol>
The first requirement implies that Qmtest may execute independent
tests in any order, and get uniform results.  It also implies that the
output of one independent test cannot be used as the input to
another.  Qmtest's design encourage programmers to write independent
tests.
</p>

<p>
A <b role="defn">fixture</b> is a starting point for a test.
Examples include:
<ul>
<li>
an empty fixture (for a function whose behavior depends only on its
input parameters);
</li>
<li>
an initial state of an object and/or a set of global variables;
</li>
<li>
one or more input files for a compiler or image processing program; or
</li>
<li>
a set of user accounts and password table entries on a networked
system.
</li>
</ul>
Creating a fixture is called <b role="defn">setup</b>; dismantling a
fixture (e.g. deleting dummy user accounts or temporary files) is
called <b role="defn">teardown</b>.
</p>

<p>
A <b role="defn">variable fixture</b> is one which may be set up in
different ways.  Typically, this involves using a pseudo-random number
generator.  As discussed in the section on <a
href="#req-reproducible">reproducibility</a>, Qmtest requires that all
variable fixtures be reproducible, i.e. that truly random data not be
used in setting up fixtures.
</p>

<p>
A <b role="defn">shared fixture</b> is one which is used as the
starting point for two or more independent tests.  Qmtest must
u(re-)create every fixture before every independent test, and tear it
down after afterward.
</p>

<p>
A <b role="defn">dependent test</b> is one which can only be
executed properly if one or more other tests have already been
executed.  Dependent tests are usually fragile than independent tests,
and provide less useful information, but are sometimes easier to set
up.
</p>

<p>
A <b role="defn">unit test</b> is a test which exercises one feature
of a test subject.  This term is necessarily as vague as the
definition of "feature".  Note that a unit test may actually interact
with the test subject many times, i.e. may call a function hundreds or
thousands of times with different parameters.  From Qmtest's point of
view, however, the result is still the same: pass, fail, error, or
deferred.
</p>

<p>
A <b role="defn">regression test</b> is one whose result is
determined by comparing its behavior or output with that obtained from
a previous run of the same test.  Regression tests are typically used
to ensure that changes to systems do not cause them to regress,
i.e. do not introduce errors in sub-systems that previously worked
correctly.  Note that the report from a regression test is only as
accurate as the inspection of the reference result.
</p>

<p>
An <b role="defn">oracle</b> is any entity which can determine the
correctness of the result of a test.  Typical oracles include:
<ul>
<li>
human experts;
</li>
<li>
independently validated versions of programs; and
</li>
<li>
manually calculated results.
</li>
</ul>
Note that reliance on human experts as oracles makes test suites
fragile, since those experts may not be available to re-assess the
correctness of reference results when they are called into question,
or to change those reference results when the test subject changes.
</p>

</a> <!-- end term -->

<a name="req">
<h2>Requirements</h2>

<p>
This section lists requirements derived from the use cases presented
above, and from the discussions on the Software Carpentry mailing
list.  The four most important requirements
(<a href="#req-accuracy">accuracy</a>, <a
href="#req-smallscale">usability</a>, <a
href="#req-reproducible">reproducibility</a>, and <a
href="#req-standards">recycling existing standards</a>) are presented
first; secondary or derived requirements follow.
</p>

<a name="req-accuracy">
<h3>Accuracy</h3>

<p>
The most important requirement on Qmtest is accuracy: it must never
report that a test has succeeded when in fact it has failed, or vice
versa: in statistical terminology, there must be <em><a
name="discuss-false-positives" role="req">no false positives</a></em>
and <em><a name="discuss-false-negatives" role="req">no false
negatives</a></em>.  A corollary of this requirement is that Qmtest
must <em><a name="discuss-execute-all" role="req">always execute at
least all of the tests requested by the user</a></em>.  Ideally, it
should only execute exactly these tests, but in practice, it is
acceptable for Qmtest to run more tests than the user asked for, so
long as all of the tests that were asked for are run.
</p>

</a> <!-- end req-accuracy -->

<a name="req-smallscale">
<h3>Small-Scale Usability</h3>

<p>
The second most important requirement is that <em><a
name="discuss-small-use" role="req">small-scale testing must be easier
to do with Qmtest than by hand</a></em>.  In particular, if Qmtest
requires users to do things that only pay off on medium or large
projects, many of those users will choose to do ad hoc testing
initially, instead of using Qmtest.  In theory, users could then
switch to Qmtest when its adoption cost was outweighed by its
benefits, but in practice, those ad hoc test suites will usually grow
(or, more likely, rust) as their projects grow.
</p>

</a> <!-- end req-smallscale -->

<a name="req-reproducible">
<h3>Reproducibility</h3>

<p>
The third requirement is that <em><a name="discuss-reproducible"
role="req">test execution must be reproducible</a></em>: Qmtest must
be able to recreate the starting point for a particular run of a test
exactly.  This requirement is necessary in order to minimize the
burden on human users: without it, it could be necessary to repeatedly
re-run a test that reported an error or failure in order to isolate
the fault.  Note that this does not mean that the test result is
exactly reproducible --- some tests of network software and user
interfaces are intrinsically non-deterministic --- but random number
generator seeds, file sizes, and the like must be stored in a
re-executable way.
</p>

</a> <!-- end req-reproducible -->

<a name="req-standards">
<h3>Recycle Existing Standards</h3>

<p>
The fourth general requirement is that Qmtest <em><a
name="discuss-recycle" role="req">must use existing standards, syntax,
conventions, and tools</a></em> wherever possible. Even when
customized solutions (e.g. a special-purpose test description
language) might be cleaner, developing, maintaining, and documenting
these solutions requires more resources than this project has.  More
importantly, the cost to users of learning, integrating, and
customizing them decreases the chances of Qmtest having an impact on
common daily practice.
</p>

<p>
The specific implications of this requirement are:
<ul>

<li>
Where Qmtest needs Boolean operators, quoting rules, floating-point
numbers, or other programmatic constructs, it uses the syntax defined
for Python.  Note that this does <em>not</em> mean that Qmtest will be
Python-centric; the rationale is simply that there is no point
defining yet another set of rules for writing things like:
<blockquote><pre>
output &lt; 5.0 and command != "start\n"
</pre></blockquote>
</li>

<li>
Where Qmtest needs structured data storage, the format of that storage
will be defined in terms of XML.  This does <em>not</em> mean that
only XML will be used: flat text, executable scripts, relational
database schemas, persisted Python scripts, and other formats may be
supported as well.  However, since XML is more constraining than these
formats, designing in terms of it will ensure that all features are
available in all modes.
</li>

<li>
Where Qmtest needs data formats (e.g. parameters for procedure calls),
it will use the rules included in the draft SOAP standard.  Again,
this does <em>not</em> mean that only SOAP will be used in the system;
as with programmatic syntax, however, there seems to be little point
developing yet another set of rules for describing or constraining
such things as arrays of strings...
</li>

</ul>
</p>

<p>
A "negative" standards requirement is that Qmtest <em><a
name="discuss-not-all-cmdline" role="req">must not require all tests
to be command-line applications</a></em>.  In particular, tests
<em>may</em> have access to standard input and standard output, and
<em>may</em> be able to report success or failure by exiting with a
zero or non-zero status, but Qmtest does not insist upon this.  This
requirement is necessary because many applications of interest may be
long-lived services (e.g. a web server), or may rely on a framework
(such as the Microsoft Foundation Classes or some Java GUI frameworks)
that does not provide standard input or standard output.
</p>

</a> <!-- end req-standards -->

<a name="req-dep">
<h3>Dependent and Independent Tests</h3>

<p>
Qmtest must <em><a name="discuss-specify-dependency" role="req">allow
programmers to specify dependencies between tests</a></em>, and must
<em><a name="discuss-track-dependency" role="req">prevent dependent
tests from being executed if an error occurred in an
antecedent</a></em>.  For obvious reasons, dependencies between tests
must be acyclic; Qmtest must <em><a name="discuss-cyclic-dependency"
role="req">detect and report cyclic dependencies</a></em>.  Qmtest
should try to <em role="desire">detect and report cyclic dependencies
before executing any tests</em>, but is not required to do this, since
this may not be possible if tests are being <a
href="#req-multi">generated on the fly</a>.
</p>

<p>
Finally, it must be possible to <em><a name="discuss-separate-prep"
role="req">separate setup and teardown from particular tests</a></em>,
so that the modified fixtures required for dependent tests are not
destroyed before those tests are executed.  Qmtest must <em><a
name="discuss-trace-dependency" role="req">trace dependencies between
tests</a></em>, and automatically determine when to set up and tear
down fixtures.  If possible, Qmtest should <em role="desire">share
dependency detection and management functionality with Qmbuild</em>.
</p>

</a> <!-- end req-dep -->

<a name="req-expect">
<h3>Expected and Unexpected Results</h3>

<p>
There are often periods in a development cycle during which certain
tests are expected to fail.  For example, if Extreme Programming's
"test, then code" development model is being used, all of a module's
tests will initially fail.  Qmtest must therefore allow programmers to
<em><a name="discuss-expect-failure" role="req">specify that some
tests are expected to fail</a></em>.  It must also <em><a
name="discuss-reported-expected-failures" role="req">report expected
failures separately from passes, unexpected failures, and other
results</a></em>.
</p>

</a> <!-- end req-expect -->

<a name="req-report">
<h3>Reporting</h3>

<p>
Qmtest must be able to <em><a name="discuss-readable-report"
role="req">report test results in textual, human-readable
form</a></em>.  This is necessary for two reasons:
<ol>
<li>
While all version control systems are able to find and display
micro-differences between text files, most are not able to do this for
binary files.  Since users should archive test results along with the
tests themselves (and the code being tested), Qmtest must be designed
to facilitate this.
</li>
<li>
Many developers still use command-line tools, and will want to run
Qmtest from the command-line.  Its output must therefore be viewable
without external helper applications, such as web browsers.
</li>
</ol>
</p>

<p>
This requirement does <em>not</em> imply that flat text will be
Qmtest's only, or usual, output format.  In particular, as XML-aware
tools mature and become more common, Qmtest must therefore be able to
<em><a name="discuss-report-xml" role="req">report and store test
results as XML</a></em>.  This will make it easier for XML-aware
integrated development environments (IDEs) and version control
systems, to leverage the semantic content of Qmtest's output.
</p>

<p>
Requiring test authors to write tests that can generate output in two
or more formats is an unacceptable burden.  Qmtest must therefore
<em><a name="discuss-format-function" role="req">provide test result
formatting and reporting functions in all languages of
interest</a></em>.  The <em><a name="discuss-format-switch"
role="req">format in which these functions report results must be
controlled by a single switch</a></em>, i.e. there cannot be separate
functions for different reporting formats.
</p>

</a> <!-- end req-report -->

<a name="req-regen">
<h3>Regeneration of Reference Results</h3>

<p>
One of the most common breakdowns in testing occurs when testers do
not check test results correctly.  If those results are then used as a
reference, against which further test runs are compared, a fault can
go undetected for a very long time.
</p>

<p>
Despite this, many (potential) Qmtest users would like it to be able
to generate reference results, as well as compare current test results
against reference versions.  This is partly a matter of convenience:
having specified what to run, and how to run it, users feel that
having to go through and run everything by hand is an unnecessary
burden.  Therefore, Qmtest <em><a name="discuss-gen-results
role="req">must be able to generate, or re-generate, reference test
results using exactly the same settings and specifications used for
testing</a></em>.
</p>

<p>
Qmtest will <em>not</em> mark programmatically-generated reference
results as "unverified", and only change their status to "verified"
when those files have been inspect by the test developer.  In
practice, almost all developers would tell Qmtest to "verify all"
immediately after generating the reference results.  This step would
therefore be as useful as putting up a dialog to ask users whether
they are sure they want to delete the files they have just marked for
deletion...
</p>

</a> <!-- end req-regen -->

<a name="req-suite">
<h3>Test Suites</h3>

<p>
In order to make test management practical, Qmtest must allow test
authors to <em><a name="discuss-test-suites" role="req">aggregate
tests into test suites</a></em>.  Further, <em><a
name="discuss-suite-prep" role="req">test suites must support setup
and teardown</a></em>, i.e. it must be possible for a suite author to
specify some initialization that is to be performed before any of the
tests in a suite are executed, and some finalization that is to be
performed after all of the suite's tests have completed.
</p>

<p>
It must be possible to <em><a name="discuss-whole-part"
role="req">make tests and test suites members of suites</a></em>.
However, <em><a name="discuss-suites-not-tests" role="req">test suites
do not generate test results</a></em>, i.e. there is no sense in which
a suite "passes" or "fails".  The reason for this is that the results
of individual tests are not binary: there is no sensible way in which
to classify an arbitrary combination of pass, fail, error, and
deferred into a scalar result.  However, as discussed below, <a
href="#req-summary">summary reporting</a> will be provided.
</p>

</a> <!-- end req-suite -->

<a name="req-id">
<h3>Test Identification</h3>

<p>
A second important aspect of test management is that <em><a
name="discuss-unique-id" role="req">every test must be uniquely
identified</a></em>, and <em><a name="discuss-id-long-lived"
role="req">test identifiers must be long-lived</a></em>.  The first
requirement is needed so that developers can specify particular tests
to be re-executed, or drill down to the results of individual tests
after the fact.  The second requirement ensures that test results can
be accurately compared against historical records.  If, for example,
tests were identified by sequence numbers within suites, then
insertion of a new test into a suite could change the numbering of
subsequent tests, which would break historical comparison.
</p>

<p>
The standard solution to the problem of unique, long-lived
identification is hierarchical naming.  Since this is familiar to most
programmers (and to any non-programmer who has ever navigated a tree
of directories and files), Qmtest will <em><a
name="discuss-hierarchical-names" role="req">require every test or
test suite to be uniquely named below its parent</a></em>.  Specific
tests can then be referred to using path-like identifiers.
</p>

<p>
As many users of backup and version control systems have discovered,
hierarchical naming does maintain historical comparability when items
are moved or renamed.  For example, if a test is moved from a
program's "I/O" suite, and put in its "runtime" suite, its full name
will change.  It would therefore be desirable for Qmtest to <em
role="desire">include name-tracking</em>, but this will not be
implemented in the first version.
</p>

</a> <!-- end req-id -->

<a name="req-summary">
<h3>Summary Reporting</h3>

<p>
Qmtest <em><a name="discuss-summarize-results" role="req">must
summarize the results of tests</a></em>, i.e. produce a count of the
number of tests which have passed, failed, generated an error, or been
deferred.  This <em><a name="discuss-hierarchical-reporting"
role="req">reporting must be hierarchical</a></em>: users must be able
to inspect the summaries for suites, sub-suites, and so on.  In order
to avoid confusion, <em><a name="discuss-summary-mirrors-hierarchy"
role="req">summary reporting must mirror the test suite
hierarchy</a></em>, i.e. aggregate results will only be produced that
correspond to the nodes in the tree of suites and sub-suites.
Finally, <em><a name="discuss-independent-reporting"
role="req">reporting and recording must be independent</a></em>,
i.e. Qmtest must be able to record more information about test results
than it provides in summary reports, so that users can drill down to
isolate faults after the fact.
</p>

</a> <!-- end req-summary -->

<a name="req-info">
<h3>Extra Information</h3>

<p>
Developers may want tests to report extra information along with a
standard result (pass, fail, error, or deferred).  For example, it may
be desirable to have tests report execution time, memory usage, number
of context switches, or code coverage.  Qmtest's <em><a
name="discuss-extra-values" role="req">output format must allow tests
to report extra named values</a></em>, while the <em><a
name="discuss-summarize-extra-values" role="req">summary reporting
system must allow extra named values to be combined
hierarchically</a></em>.
</p>

<p>
One common case in which extra information is provided is <em><a
name="discuss-explain-deferral" role="req">explaining why specific
tests were deferred</a></em>.  If test authors make the execution of
particular tests dependent on external factors, which (combination) of
those factors resulted in the test being deferred must be reported.
This topic is revisited in the discussion of <a
href="#req-traversal">test traversal</a> below.
</p>

<p>
Note that this version of Qmtest restricts extra information to be a
flat property set, i.e. name/value pairs without any internal
structure.  This restriction may be relaxed in future versions.  Note
also that collecting this extra information may require modifications
to the Qmtest execution engine.  Qmtest must therefore allow users to
<em><a name="discuss-override-engine" role="req">override all aspects
of the execution engine's internal operation</a></em>, e.g. by
deriving one or more new execution engine classes from the standard
engine class, and telling the framework to use those to run particular
tests.
</p>

</a> <!-- end req-info -->

<a name="req-change">
<h3>Logging Changes</h3>

<p>
The most important question Qmtest can answer during the development
cycle is, "What has changed?"  Qmtest's default reporting must
therefore <em><a name="discuss-report-changes" role="req">report test
results in a manner that draws attention to recent changes</a></em>.
In order to do this, Qmtest must be able to <em><a
name="discuss-temporary-records" role="req">temporarily record the
results of recent test runs</a></em>, and <em><a
name="discuss-permanent-records" role="req">permanently record the
results of selected test runs</a></em>.  The <em><a
name="discuss-user-recording-control" role="req">distinction between
temporary and permanent recording must be under developers'
control</a></em>, so that (for example) an individual developer can
create a personal log of changes to test results while trying to fix a
specific bug, and then create a permanent shared record of test
results when the bug has been fixed, or a new feature added.
</p>

</a>

<a name="req-traversal">
<h3>Traversal</h3>

<p>
Qmtest must be able to <em><a name="discuss-automatic-traversal"
role="req">automatically traverse test suites by default</a></em>,
i.e. execute all of the tests or test suites included in a suite
recursively.  However, it must be possible for users to <em><a
name="discuss-user-control-traversal" role="req">specify tests or
suites to be executed or omitted</a></em>.
</p>

<p>
Four special cases of traversal control deserve closer attention.  The
first is that the user must be able to specify that <em><a
name="discuss-include-omit" role="req">one or more tests are to be
included or omitted</a></em> when Qmtest is run.  This <em><a
name="discuss-log-filtering" role="req">filtering must be
logged</a></em>, e.g. by recording it in the header of the overall
test result record, or by including dummy deferred reports for all
tests not executed.  The reason for deferring tests must be included
in the test report, as discussed in the section on <a
href="#req-info">extra information</a>.
</p>

<p>
The second special case arises when the user makes a persistent change
to the <a href="#req-spec">test specification (discussed below)</a> to
include or exclude a test or set of tests.  This filter specification
must be logged, as discussed above.
</p>

<p>
The third special case is that Qmtest must be able to <em><a
name="discuss-filter-by-result" role="req">re-execute only those tests
that produced specific results during a previous run</a></em>.  This
implies that Qmtest <em><a name="discuss-access-temporary-results"
role="req">must have access to the temporary and permanent results of
previous test runs while executing</a></em>.
</p>

<p>
The final special case is that <em><a name="discuss-tag-tests"
role="req">users must be able to provide symbolic tags for tests or
test suites which can be used as input to filtering</a></em>.  For
example, users must be able to specify that a particular test is only
to be executed on specific platforms, or that it has a "cost" of 10
(in some arbitrary units), so that it is only run when a full
smoke-and-build test is being done.  The effect this requirement has
on <a href="#req-spec">test specification</a> is discussed below.
</p>

<p>
One common case that Qmtest's traversal mechanism must be able to
handle correctly is interruption and re-start.  A complete test suite
may take hours or days to run; users <em><a name="discuss-interrupt"
role="req">must be able to pause and re-start test execution</a></em>.
This will often be more complex than typing <code>Ctrl-Z</code> at a
command prompt, since the test harness may spawn sub-processes, or may
execute some tests in parallel on <a href="#discuss-parallel">other
machines</a>.  Note that it would also be desirable for Qmtest to be
able to <em role="desire">checkpoint and restart</em> in the event of
system failure (e.g. power outages), but this will not be included in
the first version.  (For a discussion of this topic, see the links to
James Bach's paper "Test Automation Snake Oil" in the references.)
</p>

</a> <!-- end req-traversal -->

<a name="req-physical">
<h3>Physical Organization of Tests</h3>

<p>
Qmtest <em><a name="discuss-physical-structure" role="req">cannot
mandate a physical structure for projects</a></em> in order to support
testing.  In particular, developers must be able to put tests,
temporary files, transient results, and permanent results:
<ul>
<li>
in the same directories as the test subjects;
</li>
<li>
in sub-directories of those directories;
</li>
<li>
in platform-specific directories (to avoid clashes when tests are
being executed concurrently);
</li>
<li>
in a parallel directory hierarchy; or
</li>
<li>
any combination of the above.
</li>
</ul>
</p>

<p>
Qmtest must therefore allow users to <em><a
name="discuss-specify-locations" role="req">separately specify the
location of tests, temporary files, transient results, and permanent
results</a></em>.  In order to avoid placing too great a burden on
first-time users, however, Qmtest <em><a
name="discuss-default-locations" role="req">cannot require explicit
specification of object locations</a></em>. Instead, it must have
sensible, predictable default expectations and behavior.
</p>

<p>
Finally, Qmtest <em><a name="discuss-no-redundancy" role="req">must
not require redundant specification</a></em> of locations.  For
example, the user should be able to specify once, at the root of the
testing hierarchy, that intermediate files are to be put in
<code>/tmp</code>, and that test results are to be archived in the
same directories as tests themselves.  Sub-suites and sub-tests should
then inherit this behavior.
</p>

</a> <!-- end req-physical -->

<a name="req-ctrl">
<h3>Controlling Execution</h3>

<p>
The longer a test suite takes to run, the less frequently it will be
used.  Qmtest therefore <em><a name="discuss-parallel" role="req">must
be able to execute tests concurrently</a></em> when the resources
needed to do so are available.  In order to encourage test developers
to structure tests so that they do not contain dependencies that would
inhibit concurrent execution, <em><a name="discuss-out-of-order"
role="req">unordered execution should be the default</a></em>,
although <em><a name="discuss-in-order" role="req">specifying test
order must be simple</a></em>.
</p>

<p>
Three other facilities needed by a network-aware test harness are the
ability to <em><a name="discuss-heterogeneous" role="req">run programs
on many different kinds of platforms simultaneously</a></em>, to
<em><a name="discuss-platform-log" role="req">log platform information
automatically as part of test results</a></em>, and <em><a
name="discuss-platform-placement" role="req">keep results from
sequential or concurrent runs on different platforms
separate</a></em>.
</p>

<p>
Qmtest must also allow users to <em><a name="discuss-timeout"
role="req">specify timeouts and resource limits on tests for
particular test runs</a></em>.  These limits must override any that are
built into the test specification.
</p>

</a> <!-- end req-ctrl -->

<a name="req-spec">
<h3>Test Specification</h3>

<p>
The single most important aspect of Qmtest's design is how users
actually create individual tests.  If this is difficult, then it
doesn't matter how elegant the rest of Qmtest is --- it will not be
widely used.  Some of the particular input formats which Qmtest be
able to support are listed below.
</p>

<dl>

<dt>
<em><a name="discuss-standalone-call" role="req">Single report from a
single standalone executable</a></em>:
</dt>
<dd>
The test developer writes a program that contains fixture setup, a
single call to a single test subject, and fixture teardown.  When the
program is executed, it inspects the result of the call to the test
subject (which may mean trapping errors), compares this to the result
the develoepr expected, and produces a textual report in a standard
format, from which Qmtest can extract the test result.  Note that this
case includes the case in which the "call" is actually a series of
calls, at the end of which a single test result is generated.
</dd>

<dt>
<em><a name="discuss-translate-call" role="req">Report translation from
a single standalone executable</a></em>:
</dt>
<dd>
The test developer specifies a program to be executed.  This may be a
special-purpose testing program, or it may be the test subject itself.
The program does not produce a test result directly.  Instead, the
test specification describes how to translate one or more observable
aspects of the program's behavior into a test result.  Some common
cases include:
	<ul>
	<li>
	<em><a name="discuss-translate-exit-code role="req">Translate
	a program's exit status code into a test result</a></em>,
	automatically trapping unexpected abnormal ends such as
	floating point exceptions or memory access violations.
	</li>
	<li>
	<em><a name="discuss-translate-file-diff" role="req">Compare
	the program's output stream to a reference copy</a></em>.
	This <em><a name="discuss-translate-custom-diff" role="req">
	must be able to use an exact or fuzzy comparison program
	specified by a user</a></em>.
	</li>
	<li>
	<em><a name="discuss-translate-side-effect" role="req">Inspect
	one or more of the side effects of the program's
	execution</a></em> (such as output files or changes to
	directory or file permissions) and produce a test report.
	</li>
	Run the test subject and <em><a
	name="discuss-translate-monitor" role="req">construct a report
	based on the output of a monitoring program</a></em>, such as
	a memory usage checker.
	</ul>
</dd>

<dt>
<em><a name="discuss-multiple-calls" role="req">Multiple reports from
a single executable</a></em>:
</dt>
<dd>
The test developer writes a program that contains multiple calls to
multiple test subjects.  Each call is preceded by fixture setup, and
followed by fixture teardown.  Each call results in a single test
report, so that when the test program is run, many test reports are
produced.  Note that Qmtest does <em>not</em> support multiple results
requiring translation from a single executable, i.e. it will
<em>not</em> create many test reports by comparing each of the many
files generated by an executable against a reference file.
</dd>

<dt>
<em>Multiple runs of a single subject with different options</em>:
</dt>
<dd>
All of the cases above (and below) may be repeated many times with
different options, such as different command-line arguments, input
files, configuration files, thresholds, and so on.  Qmtest's users
therefore <a name="discuss-arg-calls" role="req">must be able to
specify repetition</a> in a simple way, and <a
name="discuss-arg-calls-id" role="req">must be able to uniquely
identify members of a repetition set</a>.
</dd>

<dt>
<em><a name="discuss-direct-calls" role="req">Direct calls to the test
subject</a></em>:
</dt>
<dd>
In some cases, it may be necessary or desirable to have Qmtest itself
call the test subject.  In particular, the test subject may be a COM
or CORBA component, a JavaBean, or a function or procedure in a
dynamic language such as Perl, Python, or Ruby.  In these cases, the
test developer must be able to specify the calling calling procedure,
and have that executed directly.
</dd>

<dt>
<em><a name="discuss-embedded-calls" role="req">Filter test
specification from arbitrary files</a></em>:
</dt>
<dd>
As well as reading test specifications from its own configuration
files, Qmtest must be able to extract test specifications from
arbitrary pre-existing files.  For example, <a
href="#story-daryl">Daryl Dowhile</a> already has 1200 test
specifications embedded as specially-formatted comments in program
source files; Qmtest must allow him to recycle those files.
</dd>

</dl>

<p>
As discussed in the section on <a href="#req-traversal">traversal</a>,
test developers must be able to <a href="#discuss-tag-tests">add
arbitrary tags to filter tests</a>.  All forms of test specification
format must therefore allow for this.
</p>

</a> <!-- end req-spec -->

<a name="req-multi">
<h3>Multi-Tests and Reflective Tests</h3>

<p>
Users will often want to generate multiple tests programmatically in
cases where generating tests manually is impractical.  For example, <a
href="#story-ovide">Ovide</a> may write a small program that
generates all 169 different possible cases of two rectangles
overlapping (or failing to overlap).  There are three ways in which
this could be managed:
<ol>
<li>
Users could be required to have their test generators save results in
the <a href="#req-spec">specification format</a> described above.
This option is rejected because of storage and management
requirements: a generator which tests all combinations arithmetic
expressions involving five or fewer operators could involve millions
of test specifications.
</li>
<li>
Qmtest could allow tests to report partial success or failure,
e.g. return a vector of results rather than a scalar result.  This
option is rejected because it makes human comprehension of results
more error-prone.
</li>
<li>
Users could be required to structure multi-tests as iterators,
i.e. build a function or test program that could be called repeatedly
to create and execute tests one at a time.  This option is rejected
because writing re-entrant code (particularly re-entrant standalone
programs) is complicated.
</li>
<li>
Users could be allowed to <em><a name="discuss-test-generator"
role="req">specify that a test specification is actually a test
specification generator</a></em>, i.e. that Qmtest was to execute the
"test", then execute on the output of the "test", and so on until
actual tests were being executed.  This option is the one that Qmtest
uses, as it integrates well with the input transformation discussed in
the section on <a href="#req-spec">test specification</a>.
</li>
</ol>
</p>

<p>
Of course, allowing test specifications to generate more test
specifications on the fly opens up the possibility of infinite
recursion.  Qmtest must therefore <em><a
name="discuss-recursion-limit" role="req">have a default limit on the
depth of test generation recursion</a></em>, and <em><a
name="discuss-override-recursion-limit" role="req">allow users to
override the test generation recursion limit</a></em>.
</p>

</a> <!-- end req-multi -->

<a name="req-updown">
<h3>Setup and Teardown</h3>

<p>
Individual tests and test suites may require arbitrary setup before,
and teardown after, their execution.  In order to simplify test
management, Qmtest requires that <em><a name="discuss-prep-in-spec"
role="req">setup and teardown must be specified in the test
specification</a></em>, that <em><a name="discuss-log-prep"
role="req">setup and teardown operations must be logged</a></em> when
tests are executed, and that <em><a name="discuss-test-after-setup"
role="req">tests are only executed if their setup operations complete
successfully</a></em>.  Users are encouraged to specify teardown
operations so that they will execute correctly (i.e. return the
environment to its pre-test state) even if setup and the test itself
failed partially or completely.
</p>

<p>
In order to avoid drowning users in detail, Qmtest must log setup
failure as a reason for deferral for all tests, but <em><a
name="discuss-first-error">only report significant setup errors by
default</a></em>.  For example, if test B depends on test A, and test
A's setup fails, Qmtest should not report that test B has failed,
since the root cause is the failure in test A's setup.
</p>

</a> <!-- end req-updown -->

<a name="req-sandbox">
<h3>Sandboxing and Throttling</h3>

<p>
A <b role="defn">sandbox</b> is an environment for program execution
in which some or all of the facilities the program relies on have been
replaced with safer, or simpler, versions.  For example, a developer
could construct a sandbox for a mailing list manager by replacing the
SMTP library with one which wrote messages to a log file, and read
messages from a specially-formatted input file, rather than
interacting with the outside world.
</p>

<p>
A <b role="defn">throttle</b> is anything used to restrict a program's
execution environment, e.g. a very small heap, or a small upper bound
on the number of threads a process is allowed to spawn.  Throttles are
typically used to simulate the effects of heavy system loads: running
a program with a small heap, for example, is a good way to test how it
will behave when it needs to allocate many large data structures.
</p>

<p>
While future versions of Qmtest <em role="desire">may come packaged
with sandbox and throttle libraries</em> for specific languages and
applications, these are out of the scope of the current effort.
However, in order to make it easy for developers to use these
facilities today (if they have them), Qmtest <em><a
name="discuss-record-build-options" role="req">must be able to record
the build options of all tests</a></em>, including compiler options,
linked libraries, and so on.
</p>

</a> <!-- end req-sandbox -->

<a name="req-list">
<h3>Requirements Summary</h3>

<ol>

<li>
<a name="summary-false-positives"/>
<a href="#discuss-false-positives">No false positives.</a>
</li>

<li>
<a name="summary-false-negatives"/>
<a href="#discuss-false-negatives">No false negatives.</a>
</li>

<li>
<a name="summary-execute-all"/>
<a href="#discuss-execute-all">Always execute at least all of the
tests requested by the user.</a>
</li>

<li>
<a name="summary-small-use"/>
<a href="#discuss-small-use">Small-scale testing must be easier to do
with Qmtest than by hand.</a>
</li>

<li>
<a name="summary-reproducible"/>
<a href="#discuss-reproducible">Test execution must be
reproducible.</a>
</li>

<li>
<a name="summary-recycle"/>
<a href="#discuss-recycle">Must use existing standards, syntax,
conventions, and tools.</a>
</li>

<li>
<a name="summary-not-all-cmdline"/>
<a href="#discuss-not-all-cmdline">Must not require all tests to be
command-line applications.</a>
</li>

<li>
<a name="summary-specify-dependency"/>
<a href="#discuss-specify-dependency">Allow programmers to specify
dependencies between tests.</a>
</li>

<li>
<a name="summary-track-dependency"/>
<a href="#discuss-track-dependency">Prevent dependent tests from being
executed if an error occurred in an antecedent.</a>
</li>

<li>
<a name="summary-cyclic-dependency"/>
<a href="#discuss-cyclic-dependency">Detect and report cyclic
dependencies.</a>
</li>

<li>
<a name="summary-separate-prep"/>
<a href="#discuss-separate-prep">Separate setup and teardown from
particular tests.</a>
</li>

<li>
<a name="summary-trace-dependency"/>
<a href="#discuss-trace-dependency">Trace dependencies between
tests.</a>
</li>

<li>
<a name="summary-expect-failure"/>
<a href="#discuss-expect-failure">Specify that some tests are expected
to fail.</a>
</li>

<li>
<a name="summary-reported-expected-failures"/>
<a href="#discuss-reported-expected-failures">Report expected failures
separately from passes, unexpected failures, and other results.</a>
</li>

<li>
<a name="summary-readable-report"/>
<a href="#discuss-readable-report">Report test results in textual,
human-readable form.</a>
</li>

<li>
<a name="summary-report-xml"/>
<a href="#discuss-report-xml">Report and store test results as
XML.</a>
</li>

<li>
<a name="summary-format-function"/>
<a href="#discuss-format-function">Provide test result formatting and
reporting functions in all languages of interest.</a>
</li>

<li>
<a name="summary-format-switch"/>
<a href="#discuss-format-switch">Format in which these functions
report results must be controlled by a single switch.</a>
</li>

<li>
<a name="summary-gen-results"/>
<a href="#discuss-gen-results">Must be able to generate, or
re-generate, reference test results using exactly the same settings
and specifications used for testing.</a>
</li>

<li>
<a name="summary-test-suites"/>
<a href="#discuss-test-suites">Aggregate tests into test suites.</a>
</li>

<li>
<a name="summary-suite-prep"/>
<a href="#discuss-suite-prep">Test suites must support setup and
teardown.</a>
</li>

<li>
<a name="summary-whole-part"/>
<a href="#discuss-whole-part">Make tests and test suites members of
suites.</a>
</li>

<li>
<a name="summary-suites-not-tests"/>
<a href="#discuss-suites-not-tests">Test suites do not generate test
results.</a>
</li>

<li>
<a name="summary-unique-id"/>
<a href="#discuss-unique-id">Every test must be uniquely
identified.</a>
</li>

<li>
<a name="summary-id-long-lived"/>
<a href="#discuss-id-long-lived">Test identifiers must be
long-lived.</a>
</li>

<li>
<a name="summary-hierarchical-names"/>
<a href="#discuss-hierarchical-names">Require every test or test suite
to be uniquely named below its parent.</a>
</li>

<li>
<a name="summary-summarize-results"/>
<a href="#discuss-summarize-results">Must summarize the results of
tests.</a>
</li>

<li>
<a name="summary-hierarchical-reporting"/>
<a href="#discuss-hierarchical-reporting">Reporting must be
hierarchical.</a>
</li>

<li>
<a name="summary-summary-mirrors-hierarchy"/>
<a href="#discuss-summary-mirrors-hierarchy">Summary reporting must
mirror the test suite hierarchy.</a>
</li>

<li>
<a name="summary-independent-reporting"/>
<a href="#discuss-independent-reporting">Reporting and recording must
be independent.</a>
</li>

<li>
<a name="summary-extra-values"/>
<a href="#discuss-extra-values">Output format must allow tests to
report extra named values.</a>
</li>

<li>
<a name="summary-summarize-extra-values"/>
<a href="#discuss-summarize-extra-values">Summary reporting system
must allow extra named values to be combined hierarchically.</a>
</li>

<li>
<a name="summary-explain-deferral"/>
<a href="#discuss-explain-deferral">Explaining why specific tests were
deferred.</a>
</li>

<li>
<a name="summary-override-engine"/>
<a href="#discuss-override-engine">Override all aspects of the
execution engine's internal operation.</a>
</li>

<li>
<a name="summary-report-changes"/>
<a href="#discuss-report-changes">Report test results in a manner that
draws attention to recent changes.</a>
</li>

<li>
<a name="summary-temporary-records"/>
<a href="#discuss-temporary-records">Temporarily record the results of
recent test runs.</a>
</li>

<li>
<a name="summary-permanent-records"/>
<a href="#discuss-permanent-records">Permanently record the results of
selected test runs.</a>
</li>

<li>
<a name="summary-user-recording-control"/>
<a href="#discuss-user-recording-control">Distinction between
temporary and permanent recording must be under developers'
control.</a>
</li>

<li>
<a name="summary-automatic-traversal"/>
<a href="#discuss-automatic-traversal">Automatically traverse test
suites by default.</a>
</li>

<li>
<a name="summary-user-control-traversal"/>
<a href="#discuss-user-control-traversal">Specify tests or suites to
be executed or omitted.</a>
</li>

<li>
<a name="summary-include-omit"/>
<a href="#discuss-include-omit">One or more tests are to be included
or omitted.</a>
</li>

<li>
<a name="summary-log-filtering"/>
<a href="#discuss-log-filtering">Filtering must be logged.</a>
</li>

<li>
<a name="summary-filter-by-result"/>
<a href="#discuss-filter-by-result">Re-execute only those tests that
produced specific results during a previous run.</a>
</li>

<li>
<a name="summary-access-temporary-results"/>
<a href="#discuss-access-temporary-results">Must have access to the
temporary and permanent results of previous test runs while
executing.</a>
</li>

<li>
<a name="summary-tag-tests"/>
<a href="#discuss-tag-tests">Users must be able to provide symbolic
tags for tests or test suites which can be used as input to
filtering.</a>
</li>

<li>
<a name="summary-interrupt"/>
<a href="#discuss-interrupt">
Must be able to pause and re-start test execution.</a>
</li>

<li>
<a name="summary-physical-structure"/>
<a href="#discuss-physical-structure">Cannot mandate a physical
structure for projects.</a>
</li>

<li>
<a name="summary-specify-locations"/>
<a href="#discuss-specify-locations">Separately specify the location
of tests, temporary files, transient results, and permanent
results.</a>
</li>

<li>
<a name="summary-default-locations"/>
<a href="#discuss-default-locations">Cannot require explicit
specification of object locations.</a>
</li>

<li>
<a name="summary-no-redundancy"/>
<a href="#discuss-no-redundancy">Must not require redundant
specification.</a>
</li>

<li>
<a name="summary-parallel"/>
<a href="#discuss-parallel">Must be able to execute tests
concurrently.</a>
</li>

<li>
<a name="summary-out-of-order"/>
<a href="#discuss-out-of-order">Unordered execution should be the
default.</a>
</li>

<li>
<a name="summary-in-order"/>
<a href="#discuss-in-order">Specifying test order must be simple.</a>
</li>

<li>
<a name="summary-heterogeneous"/>
<a href="#discuss-heterogeneous">Run programs on many different kinds
of platforms simultaneously.</a>
</li>

<li>
<a name="summary-platform-log"/>
<a href="#discuss-platform-log">Log platform information automatically
as part of test results.</a>
</li>

<li>
<a name="summary-platform-placement"/>
<a href="#discuss-platform-placement">Keep results from sequential or
concurrent runs on different platforms separate.</a>
</li>

<li>
<a name="summary-timeout"/>
<a href="#discuss-timeout">Specify timeouts and resource limits on
tests for particular test runs.</a>
</li>

<li>
<a name="summary-standalone-call"/>
<a href="#discuss-standalone-call">Single report from a single
standalone executable.</a>
</li>

<li>
<a name="summary-translate-call"/>
<a href="#discuss-translate-call">Report translation from a single
standalone executable.</a>
</li>

<li>
<a name="summary-translate-file-diff"/>
<a href="#discuss-translate-file-diff">Compare the program's output
stream to a reference copy.</a>
</li>

<li>
<a name="summary-translate-custom-diff"/>
<a href="#discuss-translate-custom-diff">Must be able to use an exact
or fuzzy comparison program specified by a user.</a>
</li>

<li>
<a name="summary-translate-side-effect"/>
<a href="#discuss-translate-side-effect">Inspect one or more of the
side effects of the program's execution.</a>
</li>

<li>
<a name="summary-translate-monitor"/>
<a href="#discuss-translate-monitor">Construct a report based on the
output of a monitoring program.</a>
</li>

<li>
<a name="summary-multiple-calls"/>
<a href="#discuss-multiple-calls">Multiple reports from a single
executable.</a>
</li>

<li>
<a name="summary-arg-calls"/>
<a href="#discuss-arg-calls">Must be able to specify repetition.</a>
</li>

<li>
<a name="summary-arg-calls-id"/>
<a href="#discuss-arg-calls-id">Must be able to uniquely identify
members of a repetition set.</a>
</li>

<li>
<a name="summary-direct-calls"/>
<a href="#discuss-direct-calls">Direct calls to the test subject.</a>
</li>

<li>
<a name="summary-embedded-calls"/>
<a href="#discuss-embedded-calls">Filter test specification from
arbitrary files.</a>
</li>

<li>
<a name="summary-test-generator"/>
<a href="#discuss-test-generator">Specify that a test specification is
actually a test specification generator.</a>
</li>

<li>
<a name="summary-recursion-limit"/>
<a href="#discuss-recursion-limit">Have a default limit on the depth
of test generation recursion.</a>
</li>

<li>
<a name="summary-override-recursion-limit"/>
<a href="#discuss-override-recursion-limit">Allow users to override
the test generation recursion limit.</a>
</li>

<li>
<a name="summary-prep-in-spec"/>
<a href="#discuss-prep-in-spec">Setup and teardown must be specified
in the test specification.</a>
</li>

<li>
<a name="summary-log-prep"/>
<a href="#discuss-log-prep">Setup and teardown operations must be
logged.</a>
</li>

<li>
<a name="summary-test-after-setup"/>
<a href="#discuss-test-after-setup">Tests are only executed if their
setup operations complete successfully.</a>
</li>

<li>
<a name="summary-first-error"/>
<a href="#discuss-first-error">Only report significant setup errors by
default.</a>
</li>

<li>
<a name="summary-record-build-options"/>
<a href="#discuss-record-build-options">Must be able to record the
build options of all tests.</a>
</li>

</ol>

</a> <!-- end req-summary -->

</a> <!-- end req -->

<a name="iface">
<h2>Interface</h2>
</a> <!-- end iface -->

<a name="solve">
<h2>User Solutions</h2>

<a name="solve-bhargan">
<h3>Bhargan Basepair</h3>
</a> <!-- end solve-bhargan -->

<a name="solve-daryl">
<h3>Daryl Dowhile</h3>
</a> <!-- end solve-daryl -->

<a name="solve-glenda">
<h3>Glenda Grammar</h3>
</a> <!-- end solve-glenda -->

<a name="solve-ovide">
<h3>Ovide Overlay</h3>
</a> <!-- end solve-ovide -->

<a name="solve-tahura">
<h3>Tahura Transparency</h3>
</a> <!-- end solve-tahura -->

</a> <!-- end solve -->

<a name="arch">
<h2>Architecture</h2>

<p>
Execution: Qmtest uses visitor pattern, calling methods of test
execution class on the way into suites, between tests in the suite,
etc.  Users may override the test execution class at any level
(derivation in Python).
</p>

<p>
Input: everything is translated into, or generates, SOAP-like XML,
which then calls back into Qmtest.  This can cause Qmtest to execute
tests, to generate new tests, or to execute other test specs
(e.g. recurse into sub-directories).
</p>

<p>
Output: Qmtest maintains a list of event handlers.  Each test is run.
If an event occurs (e.g. timeout, memory fault, etc.), the appropriate
handler is invoked to translate the fault into a test report.  If no
event occurs, but a handler is associated with the test itself, Qmtest
runs that handler.  It is responsible for translating the test's exit
status, output files, stdout, etc., into a test report.  If there is
no test-specific handler, Qmtest uses the program's stdout (or, if it
cannot read or parse that, its exit status) to generate a test report.
</p>

<p>
Standard output comparators are:
<ul>
<li>
No-op (just echo stdout to Qmtest)
</li>
<li>
Compare text (with flags to handle CR/LF, Unicode, and/or sorting)
</li>
<li>
Compare bitwise
</li>
<li>
Compare floating point within tolerance (what to do about large data
sets, arrays, etc.?)
</li>
</ul>
</p>

<p>
Recording: how best to record test results?  Persisted Python, XML,
SQL database?
</p>

<p>
Should Qmtest include pre-defined configuration tags?
</p>

<p>
How to specify where tests are allowed to run?  Can't use specific
machine names in test suite itself (reduces portability).  Separate
list of machines, in separate spec?
</p>

</a> <!-- end arch -->

<a name="devplan">
<h2>Development Plan</h2>
</a> <!-- end devplan -->

<a name="risk">
<h2>Risks</h2>
</a> <!-- end risk -->

<a name="ref">
<h2>References</h2>

<p>
Brian Marick (a judge in both rounds of the Software Carpentry design
competition) is the author of a thorough survey of the art called
<cite>The Craft of Software Testing</cite>.  Many of his other
writings are available on-line at:
<blockquote>
<a href="http://www.testing.com">http://www.testing.com</a>
</blockquote>
</p>

<p>
<cite>Testing Computer Software</cite>, by Cem Kaner, Hung Quoc
Nguyen, and Jack Falk, is a good look at testing in the real world.
</p>

<p>
<cite>The Pragmatic Programmer</cite>, by Andy Hunt and Dave Thomas
(both judges in the Software Carpentry design competition), includes a
lot of good material on when and how to test during development.  The
book's web site is at:
<blockquote>
<a href="http://www.pragmaticprogrammer.com">http://www.pragmaticprogrammer.com</a>
</blockquote>
</p>

<p>
Some existing testing tools used in Open Source development include:
	<blockquote>
	<table cellpadding="5">
	<tr>
		<td>Expect</td>
		<td><a href="http://expect.nist.gov/">http://expect.nist.gov/</a></td>
	</tr>
	<tr>
		<td>DejaGnu</td>
		<td><a href="http://www.gnu.org/software/dejagnu/dejagnu.html">http://www.gnu.org/software/dejagnu/dejagnu.html</a></td>
	</tr>
	<tr>
		<td>TET</td>
		<td><a href="http://tetworks.opengroup.org/">http://tetworks.opengroup.org/</a</td>
	</tr>
	</table>
	</blockquote>
</p>

<p>
The FAQ for comp.software.testing is available at:
<blockquote>
<a href="http://www.landfield.com/faqs/software-eng/testing-faq/">http://www.landfield.com/faqs/software-eng/testing-faq/</a>.
</blockquote>
</p>

<p>
The <a href="http://www.visualizationtoolkit.org">Visualization
Toolkit</a> (VTK) stands out among large Open Source projects for the
amount and thoroughness of its testing.  The procedures VTK uses are
described in a paper by William Lorensen and James Miller, which is
available on-line at:
<blockquote>
<a href="ftp://www.visualizationtoolkit.org/pub/vtk/ExtremeTestingPaper.pdf">ftp://www.visualizationtoolkit.org/pub/vtk/ExtremeTestingPaper.pdf</a>
</blockquote>
</p>

<p>
Highlights from the <a
href="http://www.software-carpentry.com">Software Carpentry</a>
discussion list are summarized at:
<blockquote>
<a href="http://www.software-carpentry.com/sc_test/#highlights">http://www.software-carpentry.com/sc_test/#highlights</a>
</blockquote>
The entire discussion list is archived at:
<blockquote>
<a href="http://www.software-carpentry.com/lists/sc-discuss/maillist.html">http://www.software-carpentry.com/lists/sc-discuss/maillist.html</a>
</blockquote>
</p>

<p>
James Bach's paper "Test Automation Snake Oil" discusses why full
automation of testing is not possible, practical, or desirable:
<blockquote>
<a href="http://www.satisfice.com/articles/test_automation_snake_oil.pdf">http://www.satisfice.com/articles/test_automation_snake_oil.pdf</a>
</blockquote>
</p>

</a> <!-- end ref -->

<a name="notes">
<h2>Notes</h2>

<p>
Qmtest will not directly compare multiple output files from a single
executable against multiple reference files.  Users can achieve this
effect in one of two ways:
	<ol>
	<li>
	Perform the comparison inside their test program, and generate
	the test results directly.  In the worst case, this will
	involve exec'ing the diff tool that Qmtest would have used.
	</li>
	<li>
	Run the generator as part of setup for a suite, then have each
	"test" be comparison of a generated file against a reference
	image (i.e. no further executable).
	</li>
	</ol>
</p>

<p>
Qmtest will not support piped test generation, i.e. will not provide
direct support for:
<blockquote><pre>
test_generator | test_subject | report_filter
</pre></blockquote>
Users may implement this themselves (e.g. as a shell script), or may
specify that the test_generator is part of the setup for the
test_subject, and have the test_generator write data to a temporary
file.
</p>

</a> <!-- end notes -->

<a name="questions">
<h2>Questions</h2>

<p>
The discussion in the section on <a href="#req-physical">physical
organization</a> seems to imply some sort of persistent record of
inherited settings.  If a user executes an entire suite with the
results directory set to <code>./results</code>, then re-executes the
part of the suite contained in the sub-directory
<code>platform</code>, Qmtest must be able to remember that results
are supposed to go in <code>platform/results</code>.  This is
manageable, but what about the case in which the user creates a whole
new sub-directory, and runs Qmtest in there for the first time, rather
than in the root directory?  How does Qmtest know that results are
supposed to go in <code>newdir/results</code>?  Should there be some
way to re-execute the test suite from the root that doesn't actually
run the tests, but only updates control settings?  If so, does this
mean that Qmtest will store the user's settings, and the actual
(fleshed-out) settings?
</p>

<p>
In the section on <a href="#discuss-multiple-calls">multiple reports
from a single executable</a>, should we mandate that multiple tests in
a single executable be considered a test suite?
</p>

<p>
In the section on <a href="#req-updown">setup and takedown</a>, how
should setup and teardown be specified?  Options that must be included
are:
<ul>
<li>
arbitrary shell commands (which are platform-specific, and may not be
directly nestable in XML);
</li>
<li>
Python scripts (which have the same problems as shell scripts); and
</li>
<li>
extra command-line options to the test itself specifying which input
file to open, what output directory to create, etc. (in cases where
these are distinct from the options that specify the test itself).
</li>
</ul>
</p>

<p>
How are test specifications identified?  Presumably there is a single
file in each interesting directory that tells Qmtest what tests to
run, and what sub-specs to read.  These files must be flat text,
Python, and XML (at least); the first two create the third.  Must be
able to use regular expressions and/or file globs to identify other
test files, along with execution rules.
</p>

<p>
How are arbitrary actions identified?  In Python, they can be shell
script fragments in a triple-quoted string.  What about flat text
input, or XML input?  Nesting in CDATA is clunky, but external files
and escaped characters are clunkier.
</p>

<p>
How does Qmtest tell programs where to put temporary files (for
comparison)?  Alternatively, how to programs tell Qmtest where they
have put their results?
</p>

</a> <!-- end questions -->

<p>
<hr>

<table cellpadding="10">
<tr>
	<td>Author:</td>
	<td>$Author$</td>
</tr>
<tr>
	<td>Date:</td>
	<td>$Date$</td>
</tr>
<tr>
	<td>Revision:</td>
	<td>$Revision$</td>
</tr>
</table>

</body>
</html>
