<?xml version="1.0"?>
<!--

  File:   core.xml
  Author: Mark Mitchell, Greg Wilson, Alex Samuel
  Date:   2000-11-01

  Contents:
    Core architecture of qmtest.

  Copyright (C) 2000 CodeSourcery LLC.  This material may
  be distributed only subject to the terms and conditions set forth in
  the Software Carpentry Open Publication License, which is available at:

    http://www.software-carpentry.com/openpub-license.html

-->
<chapter id="req"><title>Requirements</title>

 <para>This section lists requirements derived from the use cases
 presented above, and from the discussions on the Software Carpentry
 mailing list.  The four most important requirements (<link
 linkend="req-accuracy">accuracy</link>, <link
 linkend="req-smallscale">usability</link>, <link
 linkend="req-reproducible">reproducibility</link>, and <link
 linkend="req-standards">conservatism</link>) are
 presented first; secondary or derived requirements follow.</para>

 <section id="req-accuracy">
  <title id="req-accuracy-title">Accuracy</title>

  <para>The most important requirement for &qmtest; is accuracy: it
  must never report that a test has succeeded when in fact it has
  failed, or vice versa. (In statistical terminology, there must be
  <phrase id="discuss-false-positives">no false positives</phrase> and
  <phrase id="discuss-false-negatives">no false negatives</phrase>.)  A
  corollary of this requirement is that &qmtest; must <phrase
  id="discuss-execute-all">always execute at least all of the tests
  requested by the user</phrase>.  Ideally, it should only execute
  exactly these tests, but in practice, it is acceptable for &qmtest;
  to run more tests than the user asked for, so long as all of the
  tests that were asked for are run.</para>

 </section>

 <section id="req-smallscale">
  <title id="req-smallscale-title">Small-Scale Usability</title>

  <para><phrase id="discuss-small-use">Small-scale testing must be
  easier to do with &qmtest; than by hand</phrase>.  If &qmtest;
  requires users to do things that only pay off on medium or large
  projects, many of those users will choose to do ad hoc testing
  initially, instead of using &qmtest;.  In theory, users could then
  switch to &qmtest; when its adoption cost was outweighed by its
  benefits, but in practice, those ad hoc test suites will usually
  grow (or, more likely, rust) as their projects grow.</para>

 </section>

 <section id="req-reproducible">
  <title id="req-reproducible-title">Reproducibility</title>

  <para><phrase id="discuss-reproducible">Test execution must be
  reproducible</phrase>: &qmtest; must be able to recreate the
  starting point for a particular run of a test exactly.  This
  requirement is necessary in order to minimize the burden on human
  users: without it, it could be necessary to repeatedly re-run a test
  that reported an error or failure in order to isolate the fault.
  Note that this does not mean that the test result is exactly
  reproducible&mdash;some tests of network software and user
  interfaces are intrinsically non-deterministic&mdash;but all
  controllable variables (such as random number generator seeds) must
  be stored in a re-executable way.</para>

 </section>

 <section id="req-standards">
  <title id="req-standards-title">Conservatism</title>

  <para>&qmtest; <phrase id="discuss-recycle">must use existing
  standards, syntax, conventions, and tools</phrase> wherever
  possible. Even when customized solutions (e.g. a special-purpose
  test description language) might be cleaner, developing,
  maintaining, and documenting these solutions requires more resources
  than this project has.  More importantly, the cost to users of
  learning, integrating, and customizing them decreases the chances of
  &qmtest; being adopted and maintained.</para>

  <para>The specific implications of this requirement are:
   <itemizedlist>
    <listitem>
     <para>Where &qmtest; needs Boolean operators, quoting rules,
     floating-point numbers, or other programmatic constructs, it will
     use the syntax defined for Python.  Note that this does
     <emphasis>not</emphasis> mean that &qmtest; will be
     Python-centric; the rationale is simply that there is no point
     defining yet another set of rules for writing things like:
      <programlisting>
       output &lt; 5.0 and command != "start\n"
      </programlisting>
     </para>
    </listitem>

    <listitem>
     <para>Where &qmtest; needs structured data storage, the format of
     that storage will be defined in terms of XML.  This does
     <emphasis>not</emphasis> mean that only XML will be used: flat
     text, executable scripts, relational database schemas, persisted
     Python scripts, and other formats may be supported as well.
     However, since XML is more constraining than these formats,
     designing in terms of it will ensure that all features are
     available in all modes.  </para>
    </listitem>

    <listitem>
     <para>Where &qmtest; needs data formats (e.g. parameters for
     procedure calls), it will use the rules included in the draft
     SOAP standard.  Again, this does <emphasis>not</emphasis> mean
     that only SOAP will be used in the system; as with programmatic
     syntax, however, there seems to be little point developing yet
     another set of rules for describing or constraining such things
     as arrays of strings.</para>
    </listitem>

   </itemizedlist>
  </para>

  <para>A "negative" standards requirement is that &qmtest; <phrase
  id="discuss-not-all-cmdline">must not require all tests to be
  command-line applications</phrase>.  In particular, tests
  <emphasis>may</emphasis> have access to standard input and standard
  output, and <emphasis>may</emphasis> be able to report success or
  failure by exiting with a zero or non-zero status, but &qmtest; does
  not insist upon this.  This requirement is necessary because many
  applications of interest may be long-lived services (e.g. a web
  server), or may rely on a framework (such as the Microsoft
  Foundation Classes or some Java GUI frameworks) that does not
  provide standard input or standard output.</para>

 </section>

 <section><title>Dependent and Independent Tests</title>

  <para>&qmtest; must <phrase id="discuss-specify-dependency">allow
  programmers to specify dependencies between tests</phrase>, and must
  <phrase id="discuss-track-dependency">prevent dependent tests from
  being executed if an error occurred in an antecedent</phrase>.  For
  obvious reasons, dependencies between tests must be acyclic;
  &qmtest; must <phrase id="discuss-cyclic-dependency">detect and
  report cyclic dependencies</phrase>.  &qmtest; should try to detect
  and report cyclic dependencies before executing any tests, but is
  not required to do this, since this may not be possible if tests are
  being <link linkend="req-multi">generated on the fly</link>.</para>

  <para>Finally, it must be possible to <phrase
  id="discuss-separate-prep">separate setup and teardown from
  particular tests</phrase>, so that the modified fixtures required
  for dependent tests are not destroyed before those tests are
  executed.  &qmtest; must <phrase id="discuss-trace-dependency">trace
  dependencies between tests</phrase>, and automatically determine
  when to set up and tear down fixtures.  If possible, &qmtest; should
  share dependency detection and management functionality with
  &qmbuild;.</para>

 </section>

 <section><title>Expected and Unexpected Results</title>

  <para>There are often periods in a development cycle during which
  certain tests are expected to fail.  For example, if Extreme
  Programming's "test, then code" development model is being used, all
  of a module's tests will initially fail.  &qmtest; must therefore
  allow programmers to <phrase id="discuss-expect-failure">specify
  that some tests are expected to fail</phrase>.  It must also <phrase
  id="discuss-reported-expected-failures">report expected failures
  separately from passes, unexpected failures, and other
  results</phrase>.</para>

 </section>

 <section><title>Reporting</title>

  <para>&qmtest; must be able to <phrase
  id="discuss-readable-report">report test results in textual,
  human-readable form</phrase>.  This is necessary for two reasons:
   <orderedlist>
    <listitem>
     <para>While all version control systems are able to find and
     display micro-differences between text files, most are not able
     to do this for binary files.  Since users should archive test
     results along with the tests themselves (and the code being
     tested), &qmtest; must be designed to facilitate this.
     </para>
    </listitem> 

    <listitem>
     <para>Many developers still use command-line tools, and will want
     to run &qmtest; from the command-line.  Its output must therefore
     be viewable without external helper applications, such as web
     browsers.  </para>
    </listitem>
   </orderedlist>
  </para>

  <para>This requirement does <emphasis>not</emphasis> imply that flat
  text will be &qmtest;'s only, or usual, output format.  In
  particular, as XML-aware tools mature and become more common,
  &qmtest; must therefore be able to <phrase
  id="discuss-report-xml">report and store test results as
  XML</phrase>.  This will make it easier for XML-aware integrated
  development environments (IDEs) and version control systems, to
  leverage the semantic content of &qmtest;'s output.
  </para>

  <para>Requiring test authors to write tests that can generate output
  in two or more formats is an unacceptable burden.  &qmtest; must
  therefore <phrase id="discuss-format-function">provide test result
  formatting and reporting functions in all programming languages of
  interest</phrase>.  The <phrase id="discuss-format-switch">format in
  which these functions report results must be controlled by a single
  switch</phrase>, i.e. there cannot be separate functions for
  different reporting formats. </para>

 </section>

 <section><title>Regeneration of Reference Results</title>

  <para>One of the most common breakdowns in testing occurs when
  testers do not check test results correctly.  If those results are
  then used as a reference, against which further test runs are
  compared, a fault can go undetected for a very long time.</para>

  <para>Despite this, many (potential) &qmtest; users would like it to
  be able to generate reference results, as well as compare current
  test results against reference versions.  This is partly a matter of
  convenience: having specified what to run, and how to run it, users
  feel that having to go through and run everything by hand is an
  unnecessary burden.  Therefore, &qmtest; must be able to <phrase
  id="discuss-gen-results">generate, or re-generate, reference test
  results using exactly the same settings and specifications used for
  testing</phrase>.</para>

  <para>&qmtest; will <emphasis>not</emphasis> mark
  programmatically-generated reference results as "unverified", and
  only change their status to "verified" when those files have been
  inspect by the test developer.  In practice, almost all developers
  would tell &qmtest; to "verify all" immediately after generating the
  reference results.</para>

 </section>

 <section><title>Test Suites</title>

  <para>In order to make test management practical, &qmtest; must
  allow test authors to <phrase id="discuss-test-suites">aggregate
  tests into test suites</phrase>.  Further, <phrase
  id="discuss-suite-prep">test suites must support setup and
  teardown</phrase>, i.e. it must be possible for a suite author to
  specify some initialization that is to be performed before any of
  the tests in a suite are executed, and some finalization that is to
  be performed after all of the suite's tests have completed.</para>

  <para>It must be possible to <phrase id="discuss-whole-part">make
  tests and test suites members of suites</phrase>.  However, <phrase
  id="discuss-suites-not-tests">test suites do not generate test
  results</phrase>, i.e. there is no sense in which a suite "passes"
  or "fails".  The reason for this is that the results of individual
  tests are not binary: there is no sensible way in which to classify
  an arbitrary combination of pass, fail, error, and deferred into a
  scalar result.  However, as discussed below, <link
  linkend="req-summary-reporting">summary reporting</link> will be
  provided.</para>

 </section>

 <section><title>Test Identification</title>

  <para>A second important aspect of test management is that <phrase
  id="discuss-unique-id">every test must be uniquely
  identified</phrase>, and <phrase id="discuss-id-long-lived">test
  identifiers must be long-lived</phrase>.  The first requirement is
  needed so that developers can specify particular tests to be
  re-executed, or drill down to the results of individual tests after
  the fact.  The second requirement ensures that test results can be
  accurately compared against historical records.  If, for example,
  tests were identified by sequence numbers within suites, then
  insertion of a new test into a suite could change the numbering of
  subsequent tests, which would break historical comparison.</para>

  <para>The standard solution to the problem of unique, long-lived
  identification is hierarchical naming.  Since this is familiar to
  most programmers (and to any non-programmer who has ever navigated a
  tree of directories and files), &qmtest; will <phrase
  id="discuss-hierarchical-names">require every test or test suite to
  be uniquely named below its parent</phrase>.  Specific tests can
  then be referred to using path-like identifiers.</para>

  <para>As many users of backup and version control systems have
  discovered, hierarchical naming does maintain historical
  comparability when items are moved or renamed.  For example, if a
  test is moved from a program's "I/O" suite, and put in its "runtime"
  suite, its full name will change.  It would therefore be desirable
  for &qmtest; to include name-tracking, but this will be left to the
  users' version control systems in the first release.</para>

 </section>

 <section id="req-summary-reporting">
  <title id="req-summary-reporting-title">Summary Reporting</title>

  <para>&qmtest; must <phrase id="discuss-summarize-results">summarize
  the results of tests</phrase>, i.e. produce a count of the number of
  tests which have passed, failed, generated an error, or been
  deferred.  This <phrase
  id="discuss-hierarchical-reporting">reporting must be
  hierarchical</phrase>: users must be able to inspect the summaries
  for suites, sub-suites, and so on.  In order to avoid confusion,
  <phrase id="discuss-summary-reporting-hierarchy">summary reporting
  must mirror the test suite hierarchy</phrase>, i.e. aggregate
  results will only be produced that correspond to the nodes in the
  tree of suites and sub-suites.  Finally, <phrase
  id="discuss-report-record">reporting and recording must be
  independent</phrase>, i.e. &qmtest; must be able to record more
  information about test results than it provides in summary reports,
  so that users can drill down to isolate faults after the
  fact.</para>

 </section>

 <section id="req-info">
  <title id="req-info-title">Extra Information</title>

  <para>Developers may want tests to report extra information along
  with a standard result (pass, fail, error, or deferred).  For
  example, it may be desirable to have tests report execution time,
  memory usage, number of context switches, or code coverage.
  &qmtest; <phrase id="discuss-extra-values">must allow tests to
  contain and report extra named values</phrase>, while the <phrase
  id="discuss-summarize-extra-values">summary reporting system must
  allow extra named values to be combined
  hierarchically</phrase>.</para>

  <para>One common case in which extra information is provided is to
  <phrase id="discuss-explain-deferral">explain why specific tests
  were deferred</phrase>.  If test authors make the execution of
  particular tests dependent on external factors, which (combination)
  of those factors resulted in the test being deferred must be
  reported.  This topic is revisited in the discussion of <link
  linkend="req-traversal">test traversal</link> below.</para>

  <para>Note that this version of &qmtest; restricts extra information
  to be a flat property set, i.e. name/value pairs without any
  internal structure.  This restriction may be relaxed in future
  versions.  Note also that collecting this extra information may
  require modifications to the &qmtest; execution engine.  &qmtest;
  must therefore allow users to <phrase
  id="discuss-override-engine">override all aspects of the execution
  engine's internal operation</phrase>, e.g. by deriving one or more
  new execution engine classes from the standard engine class, and
  telling the framework to use those to run particular tests.</para>

 </section>

 <section><title>Logging Changes</title>

  <para>The most important question &qmtest; can answer during the
  development cycle is, "What has changed?"  &qmtest;'s default
  reporting must therefore <phrase id="discuss-report-changes">report
  test results in a manner that draws attention to recent
  changes</phrase>.  In order to do this, &qmtest; must be able to
  <phrase id="discuss-temporary-records">temporarily record the
  results of recent test runs</phrase>, and <phrase
  id="discuss-permanent-records">permanently record the results of
  selected test runs</phrase>.  The <phrase
  id="discuss-user-recording-control">distinction between temporary
  and permanent recording must be under developers' control</phrase>,
  so that (for example) an individual developer can create a personal
  log of changes to test results while trying to fix a specific bug,
  and then create a permanent shared record of test results when the
  bug has been fixed, or a new feature added.</para>

 </section>

 <section id="req-traversal">
  <title id="req-traversal-title">Traversal</title>

  <para>&qmtest; must be able to <phrase
  id="discuss-automatic-traversal">automatically traverse test suites
  by default</phrase>, i.e. execute all of the tests or test suites
  included in a suite recursively.  However, it must be possible for
  users to <phrase id="discuss-user-control-traversal">specify tests
  or suites to be executed or omitted</phrase>.</para>

  <para>Four special cases of traversal control deserve closer
  attention.  The first is that the user must be able to <phrase
  id="discuss-include-omit">specify that one or more tests are to be
  included or omitted</phrase> when &qmtest; is run.  This <phrase
  id="discuss-log-filtering">filtering must be logged</phrase>,
  e.g. by recording it in the header of the overall test result
  record, or by including deferred reports for all tests not executed.
  The reason for deferring tests must be included in the test report,
  as discussed in the section on <link linkend="req-info">extra
  information</link>.</para>

  <para>The second special case arises when the user makes a
  persistent change to the <link linkend="req-spec">test specification
  (discussed below)</link> to include or exclude a test or set of
  tests.  This filter specification must be logged, as discussed
  above.</para>

  <para>The third special case is that &qmtest; must be able to
  <phrase id="discuss-filter-by-result">re-execute only those tests
  that produced specific results during a previous run</phrase>.  This
  implies that &qmtest; <phrase
  id="discuss-access-temporary-results">must have access to the
  temporary and permanent results of previous test runs while
  executing</phrase>.</para>

  <para>The final special case is that <link
  linkend="discuss-tag-tests">users must be able to provide symbolic
  tags for tests or test suites which can be used as input to
  filtering</link>.  For example, users must be able to specify that
  a particular test is only to be executed on specific platforms, or
  that it has a "cost" of 10 (in some arbitrary units), so that it is
  only run when a full smoke-and-build test is being done.  The effect
  this requirement has on <link linkend="req-spec">test
  specification</link> is discussed below.</para>

  <para>One common case that &qmtest;'s traversal mechanism must be
  able to handle correctly is interruption and re-start.  A complete
  test suite may take hours or days to run; users <phrase
  id="discuss-interrupt">must be able to pause and re-start test
  execution</phrase>.  This will often be more complex than typing
  <keycombo action="simul"><keycap>Ctrl</keycap>
  <keycap>Z</keycap></keycombo> at a command prompt, since the test
  harness may spawn sub-processes, or may execute some tests in
  parallel on <link linkend="discuss-parallel">other machines</link>.
  Note that it would also be desirable for &qmtest; to be able to
  checkpoint and restart in the event of system failure (e.g. power
  outages), but this will not be included in the first version.  (For
  a discussion of this topic, see the links to James Bach's paper
  "Test Automation Snake Oil" in the references.)</para>

 </section>

 <section><title>Physical Organization of Tests</title>

  <para>&qmtest; <phrase id="discuss-physical-structure">cannot
  mandate a physical structure for projects</phrase> in order to
  support testing.  In particular, developers must be able to put
  tests, temporary files, transient results, and permanent results:
   <itemizedlist>
    <listitem>
     <para>in the same directories as the test subjects;</para>
    </listitem> 

    <listitem>
     <para> in sub-directories of those directories;</para>
    </listitem> 

    <listitem>
     <para>in platform-specific directories (to avoid clashes when
     tests are being executed concurrently); </para>
    </listitem>

    <listitem>
     <para>in a parallel directory hierarchy; or </para>
    </listitem>
  
    <listitem>
     <para>any combination of the above.</para>
    </listitem>
   </itemizedlist>
  </para>

  <para>&qmtest; must therefore allow users to <phrase
  id="discuss-specify-locations">separately specify the location of
  tests, temporary files, transient results, and permanent
  results</phrase>.  In order to avoid placing too great a burden on
  first-time users, however, &qmtest; <phrase
  id="discuss-default-locations">cannot require explicit specification
  of object locations</phrase>. Instead, it must have sensible,
  predictable default expectations and behavior.</para>

  <para>Finally, &qmtest; <phrase id="discuss-no-redundancy">must not
  require redundant specification</phrase> of locations.  For example,
  the user should be able to specify once, at the root of the testing
  hierarchy, that intermediate files are to be put in
  <filename>/tmp</filename>, and that test results are to be archived
  in the same directories as tests themselves.  Sub-suites and
  sub-tests should then inherit this behavior.</para>

 </section>

 <section><title>Controlling Execution</title>

  <para>The longer a test suite takes to run, the less frequently it
  will be used.  &qmtest; therefore <phrase id="discuss-parallel">must
  be able to execute tests concurrently</phrase> when the resources
  needed to do so are available.  In order to encourage test
  developers to structure tests so that they do not contain
  dependencies that would inhibit concurrent execution, <phrase
  id="discuss-out-of-order">unordered execution should be the
  default</phrase>, although <phrase id="discuss-in-order">specifying
  test order must be simple</phrase>.</para>

  <para>Three other facilities needed by a network-aware test harness
  are the ability to <phrase id="discuss-heterogeneous">run programs
  on many different kinds of platforms simultaneously</phrase>, to
  <phrase id="discuss-platform-log">log platform information
  automatically as part of test results</phrase>, and <phrase
  id="discuss-platform-placement">keep results from sequential or
  concurrent runs on different platforms separate</phrase>.</para>

  <para>&qmtest; must also allow users to <phrase id="discuss-timeout"
  >specify timeouts and resource limits on tests for particular test
  runs</phrase>.  These limits must override any that are built into
  the test specification. </para>

 </section>

 <section id="req-spec">
  <title>Test Specification</title>

  <para>The single most important aspect of &qmtest;'s design is how
  users actually create individual tests.  If this is difficult, then
  it doesn't matter how elegant the rest of &qmtest; is&mdash;it will
  not be widely used.  Some of the particular input formats which
  &qmtest; must be able to support are listed below.</para>

  <glosslist>

   <glossentry>
    <glossterm id="discuss-standalone-call">Single report from a
    single standalone executable</glossterm>
    <glossdef>
     <para>The test developer writes a program that contains fixture
     setup, a single call to a single test subject, and fixture
     teardown.  When the program is executed, it inspects the result
     of the call to the test subject (which may mean trapping errors),
     compares this to the result the developer expected, and produces
     a textual report in a standard format, from which &qmtest; can
     extract the test result.  Note that this case includes the case
     in which the "call" is actually a series of calls, at the end of
     which a single test result is generated.</para>
    </glossdef>
   </glossentry>

   <glossentry>
    <glossterm id="discuss-translate-call">Report translation from
    a single standalone executable</glossterm>
    <glossdef>
     <para>The test developer specifies a program to be executed.
     This may be a special-purpose testing program, or it may be the
     test subject itself.  The program does not produce a test result
     directly.  Instead, the test specification describes how to
     translate one or more observable aspects of the program's
     behavior into a test result.  Some common cases include:
      <itemizedlist>
       <listitem>
        <para><phrase id="discuss-translate-exit-code">Translate a
        program's exit status code into a test result</phrase>,
        automatically trapping unexpected abnormal ends such as
        floating point exceptions or memory access violations.</para>
       </listitem>

       <listitem>
        <para><phrase id="discuss-translate-file-diff">Compare the
        program's output to a reference copy.</phrase> This <phrase
        id="discuss-translate-custom-diff">must be able to use an
        user-specified comparator to determine whether a program's
        output matches a reference result</phrase>.</para>
       </listitem>

       <listitem>
        <para><phrase id="discuss-translate-side-effect">Inspect one
        or more of the side effects of the program's
        execution</phrase> (such as output files or changes to
        directory or file permissions) and produce a test
        report. </para>
       </listitem>

       <listitem>
        <para>Run the test subject and <phrase
        id="discuss-translate-monitor">construct a report based on the
        output of a monitoring program</phrase>, such as a memory
        usage checker.</para>
       </listitem>
      </itemizedlist>
     </para>
    </glossdef>
   </glossentry>

   <glossentry>
    <glossterm id="discuss-multiple-calls">Multiple reports from a
    single executable</glossterm> 
    <glossdef>
     <para>The test developer writes a program that contains multiple
     calls to multiple test subjects.  Each call is preceded by
     fixture setup, and followed by fixture teardown.  Each call
     results in a single test report, so that when the test program is
     run, many test reports are produced.  Note that &qmtest; does
     <emphasis>not</emphasis> support multiple results requiring
     translation from a single executable, i.e. it will
     <emphasis>not</emphasis> create many test reports by comparing
     each of the many files generated by an executable against a
     reference file (although a user-defined result comparator may do
     so).</para>
    </glossdef>
   </glossentry>

   <glossentry>
    <glossterm>Multiple runs of a single subject with different
    options</glossterm>
    <glossdef>
     <para>All of the cases above (and below) may be repeated many
     times with different options, such as different command-line
     arguments, input files, configuration files, thresholds, and so
     on.  &qmtest;'s users therefore <phrase
     id="discuss-arg-calls">must be able to specify
     repetition</phrase> in a simple way, and <phrase
     id="discuss-arg-calls-id">must be able to uniquely identify
     members of a repetition set</phrase>.</para>
    </glossdef>
   </glossentry>

   <glossentry>
    <glossterm id="discuss-direct-calls">Direct calls to the test
    subject</glossterm>
    <glossdef>
     <para>In some cases, it may be necessary or desirable to have
     &qmtest; itself call the test subject.  In particular, the test
     subject may be a COM or CORBA component, a JavaBean, or a
     function or procedure in a dynamic language such as Perl, Python,
     or Ruby.  In these cases, the test developer must be able to
     specify the calling calling procedure, and have that executed
     directly.</para>
    </glossdef>
   </glossentry>

   <glossentry>
    <glossterm id="discuss-embedded-calls">Filter test
    specification from arbitrary files</glossterm>
    <glossdef>
     <para>As well as reading test specifications from its own
     configuration files, &qmtest; must be able to extract test
     specifications from arbitrary pre-existing files.  For example,
     if Daryl Dowhile already has 1200 test specifications embedded as
     specially-formatted comments in program source files; &qmtest;
     must allow him to recycle those files.</para>
    </glossdef>
   </glossentry>

  </glosslist>

  <para>As discussed in the section on <link
  linkend="req-traversal">traversal</link>, test developers must be
  able to <phrase id="discuss-tag-tests">add arbitrary tags to
  filter tests</phrase>.  All forms of test specification format must
  therefore allow for this. </para>

 </section>

 <section id="req-multi">
  <title>Multi-Tests and Reflective Tests</title>

  <para>Users will often want to generate multiple tests
  programmatically in cases where generating tests manually is
  impractical.  For example, <link linkend="story-ovide">Ovide</link>
  may write a small program that generates all 169 different possible
  cases of two rectangles overlapping (or failing to overlap).  There
  are three ways in which this could be managed:
   <orderedlist>
    <listitem>
     <para>Users could be required to have their test generators save
     results in the <link linkend="req-spec">specification
     format</link> described above.  This option is rejected because
     of storage and management requirements: a generator which tests
     all combinations arithmetic expressions involving five or fewer
     operators could involve millions of test specifications.  </para>
    </listitem>
  
    <listitem>
     <para>&qmtest; could allow tests to report partial success or
     failure, e.g. return a vector of results rather than a scalar
     result.  This option is rejected because it makes human
     comprehension of results more error-prone.</para>
    </listitem>
  
    <listitem>
     <para>Users could be required to structure multi-tests as
     iterators, i.e. build a function or test program that could be
     called repeatedly to create and execute tests one at a time.
     This option is rejected because writing re-entrant code
     (particularly re-entrant standalone programs) is
     complicated.</para>
    </listitem>
  
    <listitem>
     <para> Users could be allowed to <phrase
     id="discuss-test-generator">specify that a test specification is
     actually a test specification generator</phrase>, i.e. that
     &qmtest; was to execute the "test", then execute on the output of
     the "test", and so on until actual tests were being executed.
     This option is the one that &qmtest; uses, as it integrates well
     with the input transformation discussed in the section on <link
     linkend="req-spec">test specification</link>.</para>
    </listitem>
   </orderedlist>
  </para>

  <para>Of course, allowing test specifications to generate more test
  specifications on the fly opens up the possibility of infinite
  recursion.  &qmtest; must therefore <phrase
  id="discuss-recursion-limit">have a default limit on the depth of
  test generation recursion</phrase>, and <phrase
  id="discuss-override-recursion-limit">allow users to override the
  test generation recursion limit</phrase>.</para>

 </section>

 <section id="req-updown">
  <title id="req-updown-title">Setup and Teardown</title>

  <para>Individual tests and test suites may require arbitrary setup
  before, and teardown after, their execution.  In order to simplify
  test management, &qmtest; requires that <phrase
  id="discuss-prep-in-spec">setup and teardown must be specified in
  the test specification</phrase>, that <phrase
  id="discuss-log-prep">setup and teardown operations must be
  logged</phrase> when tests are executed, and that <phrase
  id="discuss-test-after-setup">tests are only executed if their setup
  operations complete successfully</phrase>.  Users are encouraged to
  specify teardown operations so that they will execute correctly
  (i.e. return the environment to its pre-test state) even if setup
  and the test itself failed partially or completely.</para>

  <para>In order to avoid drowning users in detail, &qmtest; must log
  setup failure as a reason for deferral for all tests, but <phrase
  id="discuss-first-error">only report significant setup errors by
  default</phrase>.  For example, if test B depends on test A, and
  test A's setup fails, &qmtest; should not report that test B has
  failed, since the root cause is the failure in test A's
  setup.</para>

 </section>

 <section id="req-sandbox">
  <title id="req-sandbox-title">Sandboxing and Throttling</title>

  <para>A <firstterm>sandbox</firstterm> is an environment for program
  execution in which some or all of the facilities the program relies
  on have been replaced with safer, or simpler, versions.  For
  example, a developer could construct a sandbox for a mailing list
  manager by replacing the SMTP library with one which wrote messages
  to a log file, and read messages from a specially-formatted input
  file, rather than interacting with the outside world.</para>

  <para>A <firstterm>throttle</firstterm> is anything used to restrict
  a program's execution environment, e.g. a very small heap, or a
  small upper bound on the number of threads a process is allowed to
  spawn.  Throttles are typically used to simulate the effects of
  heavy system loads: running a program with a small heap, for
  example, is a good way to test how it will behave when it needs to
  allocate many large data structures.</para>

  <para>While future versions of &qmtest; may come packaged with
  sandbox and throttle libraries for specific languages and
  applications, these are out of the scope of the current effort.
  However, in order to make it easy for developers to use these
  facilities today (if they have them), &qmtest; <phrase
  id="discuss-record-build-options">must be able to record the build
  options of all tests</phrase>, including compiler options, linked
  libraries, and so on.</para>

 </section>


 <section id="req-summary">
  <title id="req-summary-title">Summary</title>

  <orderedlist>

  <listitem>
  <!-- phrase id="discuss-false-positives"-- >
  <para>No false positives.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-false-negatives" -->
  <para>No false negatives.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-execute-all" -->
  <para>Always execute at least all of the tests requested by the user.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-small-use" -->
  <para>Small-scale testing must be easier to do with &qmtest; 
        than by hand.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-reproducible" -->
  <para>Test execution must be reproducible.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-recycle" -->
  <para>Must use existing standards, syntax, conventions, and tools.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-not-all-cmdline" -->
  <para>Must not require all tests to be command-line applications.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-specify-dependency" -->
  <para>Allow programmers to specify dependencies between tests.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-track-dependency" -->
  <para>Prevent dependent tests from being executed if an error occurred in
  an antecedent.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-cyclic-dependency" -->
  <para>detect and
  Report cyclic dependencies.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-separate-prep" -->
  <para>Separate setup and teardown from particular tests.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-trace-dependency" -->
  <para>Trace dependencies between tests.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-expect-failure" -->
  <para>Specify that some tests are expected to fail.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-reported-expected-failures" -->
  <para>Report expected failures separately from passes, unexpected
  failures, and other results.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-readable-report" -->
  <para>Report test results in textual, human-readable form.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-report-xml" -->
  <para>Report and store test results as XML.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-format-function" -->
  <para>Provide test result formatting and reporting functions in all
  programming languages of interest.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-format-switch" -->
  <para>Format in which these functions report results must be controlled by
  a single switch.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-gen-results" -->
  <para>Generate, or re-generate, reference test results using exactly the
  same settings and specifications used for testing.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-test-suites" -->
  <para>Aggregate tests into test suites.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-suite-prep" -->
  <para>Test suites must support setup and teardown.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-whole-part" -->
  <para>Make tests and test suites members of suites.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-suites-not-tests" -->
  <para>Test suites do not generate test results.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-unique-id" -->
  <para>Every test must be uniquely identified.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-id-long-lived" -->
  <para>Test identifiers must be long-lived.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-hierarchical-names" -->
  <para>Require every test or test suite to be uniquely named below its
  parent.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-summarize-results" -->
  <para>Summarize the results of tests.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-hierarchical-reporting" -->
  <para>Reporting must be hierarchical.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-summary-reporting-hierarchy" -->
  <para>Summary reporting must mirror the test suite hierarchy</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-report-record" -->
  <para>Reporting and recording must be independent.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-extra-values" -->
  <para>Must allow tests to contain and report extra named values.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-summarize-extra-values" -->
  <para>Summary reporting system must allow extra named values to be
  combined hierarchically.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-explain-deferral" -->
  <para>Explain why specific tests were deferred.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-override-engine" -->
  <para>Override all aspects of the execution engine's internal operation.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-report-changes" -->
  <para>Report test results in a manner that draws attention to recent
  changes.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-temporary-records" -->
  <para>Temporarily record the results of recent test runs.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-permanent-records" -->
  <para>  Permanently record the results of selected test runs.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-user-recording-control" -->
  <para>Distinction between temporary and permanent recording must be under
  developers' control.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-automatic-traversal" -->
  <para>Automatically traverse test suites by default.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-user-control-traversal" -->
  <para>Specify tests or suites to be executed or omitted.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-include-omit" -->
  <para>Specify that one or more tests are to be included or omitted.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-log-filtering" -->
  <para>Filtering must be logged.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-filter-by-result" -->
  <para>Re-execute only those tests that produced specific results during a
  previous run.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-access-temporary-results" -->
  <para>Must have access to the temporary and permanent results of previous
  test runs while executing.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-interrupt" -->
  <para>Must be able to pause and re-start test execution.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-physical-structure" -->
  <para>Cannot mandate a physical structure for projects.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-specify-locations" -->
  <para>Separately specify the location of tests, temporary files, transient
  results, and permanent results.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-default-locations" -->
  <para>Cannot require explicit specification of object locations.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-no-redundancy" -->
  <para>Must not require redundant specification.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-parallel" -->
  <para>Must be able to execute tests concurrently.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-out-of-order" -->
  <para>Unordered execution should be the default.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-in-order" -->
  <para>Specifying test order must be simple.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-heterogeneous" -->
  <para>Run programs on many different kinds of platforms simultaneously.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-platform-log" -->
  <para>Log platform information automatically as part of test results.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-platform-placement" -->
  <para>Keep results from sequential or concurrent runs on different
  platforms separate.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-timeout" -- > 
  <para>Specify timeouts and resource limits on tests for particular test
  runs.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-translate-exit-code" -->
  <para>Translate a program's exit status code into a test result.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-translate-file-diff" -->
  <para>Compare the program's output to a reference copy..</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-translate-custom-diff" -->
  <para>Must be able to use an user-specified comparator to determine
  whether a program's output matches a reference result.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-translate-side-effect" -->
  <para>Inspect one or more of the side effects of the program's execution.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-translate-monitor" -->
  <para>Construct a report based on the output of a monitoring program.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-arg-calls" -->
  <para>Must be able to specify repetition.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-arg-calls-id" -->
  <para>Must be able to uniquely identify members of a repetition set.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-tag-tests" -->
  <para>Add arbitrary tags to filter tests.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-test-generator" -->
  <para>Specify that a test specification is actually a test specification
  generator.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-recursion-limit" -->
  <para>Have a default limit on the depth of test generation recursion.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-override-recursion-limit" -->
  <para>Allow users to override the test generation recursion limit.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-prep-in-spec" -->
  <para>Setup and teardown must be specified in the test specification.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-log-prep" -->
  <para>Setup and teardown operations must be logged.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-test-after-setup" -->
  <para>Tests are only executed if their setup operations complete
  successfully.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-first-error" -->
  <para>Only report significant setup errors by default.</para>
  <!-- /phrase -->
  </listitem>

  <listitem>
  <!-- phrase id="discuss-record-build-options" -->
  <para>Must be able to record the build options of all tests.</para>
  <!-- /phrase -->
  </listitem>

  </orderedlist>

 </section>

</chapter>

<!--
  Local Variables:
  mode: sgml
  indent-tabs-mode: nil
  sgml-indent-step: 1 
  sgml-always-quote-attributes: t
  sgml-general-insert-case: lower
  sgml-minimize-attributes: nil
  sgml-parent-document: ("qmtest.xml" "book" "chapter")
  End:
-->
